# 专业选择指导应用需求设计文档

## 项目概述

### 项目简介

本项目是一款智能专业选择指导应用，旨在通过AI语音助手为高中生及其家长提供个性化的专业选择指导服务。应用支持多端访问（Web、iOS、Android、小程序），用户可以通过语音与智能助手进行自然交流，获取专业信息、学科趋势分析、院校推荐等信息。系统具备智能信息爬取、数据可视化、讲解视频生成、邮件推送等功能，帮助学生在人生重要抉择中获得清晰的方向。

### 项目目标

1. **核心目标**：打造一款真正能够帮助高中生解决专业选择困惑的智能应用，让学生在专业选择上不再迷茫，找到适合自己的发展方向
2. **用户体验目标**：建立信任感和亲和力，像朋友一样与学生交流，善于倾听和引导，给予温暖的支持
3. **功能目标**：提供完整的专业选择服务链条，从信息获取、分析推荐到最终决策支持
4. **技术目标**：构建可扩展、高可用的微服务架构，支持多端接入和海量用户并发

### 目标用户

| 用户群体 | 需求特点 |
|---------|---------|
| 高中毕业生 | 缺乏了解、需要引导、决策焦虑 |
| 高中生家长 | 希望了解专业前景、协助孩子决策 |
| 教育工作者 | 需要工具辅助学生指导工作 |

### 核心功能模块

| 模块名称 | 功能描述 | 重要性 |
|---------|---------|--------|
| 语音交互模块 | 支持语音输入和语音输出，实现自然语言对话 | 核心 |
| 智能助手引擎 | 基于大模型的对话系统，具备专业知识和心理引导能力 | 核心 |
| 信息爬取引擎 | 自动爬取网络上的专业信息、院校信息、行业趋势 | 核心 |
| 推荐算法引擎 | 基于用户画像的学科/专业推荐系统 | 核心 |
| 数据可视化模块 | 专业趋势和就业行情的图表展示 | 重要 |
| 视频生成模块 | 生成专业讲解视频 | 重要 |
| 邮件服务模块 | 专业信息推送和报告发送 | 重要 |
| 用户管理模块 | 用户注册、登录、偏好设置、学生档案 | 基础 |
| 多端适配层 | Web、iOS、Android、小程序支持 | 基础 |

---

## 文档规范

### 语言规范

本项目所有文档、代码注释、提交信息均使用**中文**。

| 类型 | 语言要求 | 示例 |
|------|---------|------|
| 项目说明文档 | 中文 | "专业选择指导应用需求设计文档" |
| 模块文档 | 中文 | "爬虫服务 - 专业行情数据爬取" |
| 代码注释 | 中文 | "# 保存数据（自动去重和数量限制）" |
| Git提交信息 | 中文 | "feat(crawler): 新增专业行情自动爬取功能" |
| API接口描述 | 中文 | "获取专业行情数据" |
| 错误信息 | 中文 | "任务不存在" |
| 用户界面文本 | 中文 | "开始对话"、"查看专业" |

### Agent和Skill文件规范

本项目所有Agent和Skill必须使用**Markdown (.md)** 格式，禁止使用Python (.py) 格式。

#### 文件格式要求

| 类型 | 格式要求 | 文件位置 | 说明 |
|------|---------|----------|------|
| Agent描述 | `.md` 格式 | `agents/` 目录 | 每个Agent一个Markdown文件 |
| Skill描述 | `.md` 格式 | `.opencode/skill/` 目录 | 每个Skill一个Markdown文件 |
| 禁止格式 | `.py` 格式 | 禁止使用 | Agent/Skill禁止使用Python文件 |

#### 目录结构规范

```
项目根目录/
├── agents/                          # Agent描述文件目录
│   ├── document-processor.md        # 文档处理器Agent
│   ├── video-processor.md           # 视频处理器Agent
│   └── ...
│
└── .opencode/
    └── skill/                       # Skill描述文件目录
        ├── document-processor/
        │   ├── doc_analyze.md       # 文档分析Skill
        │   ├── doc_crawler.md       # 文档爬取Skill
        │   ├── doc_generate.md      # 文档生成Skill
        │   └── doc_summarize.md     # 文档总结Skill
        │
        └── video-processor/
            ├── video_search.md      # 视频搜索Skill
            ├── video_summary.md     # 视频总结Skill
            └── ...
```

#### Skill文件模板

每个Skill必须包含以下内容：

```markdown
# Skill名称

## 描述
简要说明这个Skill的功能和用途

## 触发词
触发使用这个Skill的关键词

## 核心能力
列出该Skill的主要功能点

## 输入参数
| 参数名 | 类型 | 必填 | 说明 |
|-------|------|------|------|
| param1 | string | 是 | 参数说明 |

## 输出格式
描述返回结果的格式

## 使用示例
提供使用示例

## 与其他Skill的配合
说明与其他Skill的协作方式
```

#### 错误示例

```python
# ❌ 禁止：使用Python文件定义Agent/Skill
# agents/document-processor.py
def analyze_document():
    pass

# ❌ 禁止：使用Python文件定义Skill
# .opencode/skill/document-processor/doc_summarize.py
def summarize():
    pass
```

#### 正确示例

```markdown
# ✅ 正确：使用Markdown文件定义Skill
# .opencode/skill/document-processor/doc_summarize.md
# 文档智能总结技能

## 描述
对文档内容进行智能分析和总结...

## 核心能力
- 文档结构分析
- 关键信息提取
- 内容摘要生成
```

#### 迁移旧文件

如果项目中有遗留的 `.py` 格式Agent/Skill文件，需要迁移到 `.md` 格式：

1. 将 `agents/` 目录下的 `.py` 文件转换为 `.md` 文件
2. 将 `.opencode/skill/` 目录下的 `.py` 文件转换为 `.md` 文件
3. 删除所有 `.py` 格式的Agent/Skill文件
4. 确保新的 `.md` 文件包含完整的功能描述和示例

#### 验证命令

```bash
# 检查是否存在.py格式的Agent文件
find agents -name "*.py" -type f

# 检查是否存在.py格式的Skill文件
find .opencode/skill -name "*.py" -type f

# 如果有输出，说明存在违规文件，需要迁移
```

### Git提交信息规范

```
<type>(<scope>): <subject>

<body>（可选）

<footer>（可选）
```

**type类型（中文）**：

| type | 说明 | 示例 |
|------|------|------|
| feat | 新功能 | feat(crawler): 新增专业行情爬取功能 |
| fix | 修复bug | fix(user): 修复登录超时问题 |
| docs | 文档更新 | docs: 更新需求设计文档 |
| style | 代码格式 | style: 格式化代码缩进 |
| refactor | 重构 | refactor(chat): 重构对话管理逻辑 |
| perf | 性能优化 | perf: 优化数据库查询 |
| test | 测试 | test: 添加单元测试 |
| chore | 构建/工具 | chore: 更新依赖版本 |

**scope范围（可选）**：

- 前端相关：frontend, web, mobile, miniprogram
- 后端相关：backend, api-gateway, user-service, chat-service, crawler-service
- 数据相关：data, analytics
- 基础设施：devops, docker, ci

**示例**：

```
feat(crawler): 新增专业行情爬取功能

- 定时执行数据爬取
- 实现数据去重和数量限制
```

---

## 数据规范

### 数据真实性原则

本项目所有页面展示的数据必须来自后端API，**禁止使用假数据或模拟数据**。

| 原则 | 说明 |
|------|------|
| **数据来源** | 所有页面数据必须调用后端API获取 |
| **禁止假数据** | 前端代码中不得包含硬编码的模拟数据 |
| **渐进降级** | API不可用时，显示加载中或空状态，不得使用假数据 |
| **真实可靠** | 数据由爬虫服务定期从权威网站爬取，确保准确性 |

### 数据流规范

```
浏览器请求
    ↓
前端API调用
    ↓
后端服务（crawler-service等）
    ↓
数据库/爬虫获取
    ↓
返回真实数据
```

### 前端数据获取规范

| 场景 | 处理方式 |
|------|----------|
| API可用 | 显示真实数据 |
| API不可用 | 显示"加载中..."或空状态 |
| API超时 | 显示重试按钮 |
| 数据为空 | 显示"暂无数据"提示 |

### 禁止使用的假数据

```javascript
// ❌ 禁止：前端硬编码假数据
const majors = [
  { name: '计算机科学', category: '工学', employmentRate: '95%' },
  ...
]

// ✅ 正确：从API获取真实数据
const response = await fetch('/api/v1/major/market-data');
const data = await response.json();
```

### API接口清单

| 接口 | 功能 | 数据来源 |
|------|------|---------|
| GET /api/v1/major/categories | 学科分类列表 | crawler-service |
| GET /api/v1/major/market-data | 专业行情数据 | crawler-service |
| GET /api/v1/chat/conversations | 对话列表 | chat-service |
| GET /api/v1/users/profile | 用户档案 | user-service |

### 专业列表排序规则

专业推荐列表按照**专业热门程度和未来发展趋势**进行排序，核心排序指标为**热度指数（heat_index）**。

#### 排序原则

| 排序维度 | 字段 | 说明 | 优先级 |
|---------|------|------|--------|
| **热度指数** | heat_index | 综合反映专业的热门程度和未来趋势 | 默认优先 |
| 就业率 | employment_rate | 专业毕业生就业比例 | 可选排序 |
| 平均薪资 | avg_salary | 专业毕业生平均薪资水平 | 可选排序 |
| 爬取时间 | crawled_at | 数据更新时间的倒序 | 兜底排序 |

#### 热度指数（heat_index）计算模型

热度指数是综合多个维度的加权评分，反映专业的热门程度和未来发展趋势：

```
heat_index = w1 × 搜索热度 + w2 × 就业率 + w3 × 薪资水平 + w4 × 发展趋势 + w5 × 招生规模
```

| 权重参数 | 含义 | 默认值 |
|---------|------|--------|
| w1 | 搜索热度权重 | 0.30 |
| w2 | 就业率权重 | 0.25 |
| w3 | 薪资水平权重 | 0.20 |
| w4 | 发展趋势权重（基于trend_data） | 0.15 |
| w5 | 招生规模权重 | 0.10 |

#### 默认排序规则

1. **首次进入页面**：默认按热度指数（heat_index）降序排列，热度最高的排在前面
2. **用户切换排序**：可选择按"综合排序"、"就业率"、"薪资"、"热度"进行排序
3. **趋势参考**：热度指数上升趋势的专业会获得更高的排名权重

#### 前端排序选项

```typescript
const SORT_OPTIONS = [
  { value: 'heat_index', label: '综合排序（热度优先）' },  // 默认
  { value: 'employmentRate', label: '就业率' },
  { value: 'avgSalary', label: '薪资' },
  { value: 'crawled_at', label: '最新更新' }
];
```

#### 后端API排序支持

```yaml
GET /api/v1/major/market-data
Query Parameters:
  - sort_by: string (可选，排序字段: heat_index, employment_rate, crawled_at, avg_salary)
  - order: string (可选，排序方向: desc, asc)
  - category: string (可选，学科门类筛选)
  - page: int
  - page_size: int

Example:
  # 默认按热度降序
  GET /api/v1/major/market-data
  
  # 按热度降序（明确指定）
  GET /api/v1/major/market-data?sort_by=heat_index&order=desc
  
  # 按就业率降序
  GET /api/v1/major/market-data?sort_by=employment_rate&order=desc
```

#### 数据展示顺序示例

| 排名 | 专业名称 | 学科 | 热度指数 | 就业率 | 薪资 |
|-----|---------|------|---------|--------|------|
| 1 | 人工智能 | 工学 | 99.1 | 98.2% | 25K-35K/月 |
| 2 | 计算机科学与技术 | 工学 | 98.5 | 95.5% | 18K-25K/月 |
| 3 | 临床医学 | 医学 | 96.5 | 100.0% | 15K-30K/月 |
| 4 | 数据科学与大数据技术 | 理学 | 94.5 | 92.3% | 20K-30K/月 |
| 5 | 软件工程 | 工学 | 92.3 | 94.2% | 18K-28K/月 |

#### 实现要求

1. **后端实现**：
   - API默认排序字段改为 `heat_index`
   - 热度指数为空的记录排在后面
   - 支持多维度排序组合

2. **前端实现**：
   - 默认使用热度指数排序
   - 排序选择器展示清晰的文案说明
   - 当前排序状态有视觉标识

3. **数据质量**：
   - 确保每个专业都有热度指数
   - 热度指数定期更新（建议每7天重新计算）

---

## 设计思路

### 业务理解与分析

通过对项目描述的深入分析，我们识别出以下核心业务特征：

1. **语音交互为核心交互方式**：用户主要通过语音与应用进行交流，这要求系统具备高质量的语音识别（ASR）、语音合成（TTS）能力，以及流畅的语音对话管理能力。语音助手不是简单的问答机器，而是具备专业选择知识和心理洞察能力的智能伙伴。

2. **专业信息智能处理**：系统需要从海量网络信息中提取有价值的数据，包括专业设置、课程内容、就业方向、薪资水平、发展趋势等。这涉及到网络爬虫、自然语言处理（NLP）、数据清洗和结构化等复杂技术栈。

3. **个性化推荐与引导**：每个学生的情况不同，系统需要建立用户画像，理解学生的学科优势、兴趣爱好、性格特点、家庭背景等多维度信息，然后提供个性化的专业推荐和建议。推荐过程应该是引导式的，逐步帮助学生明确自己的方向。

4. **数据可视化与视频输出**：抽象的专业数据和趋势需要通过直观的图表和视频来呈现，让学生能够轻松理解不同专业的情况和发展前景。视频生成需要结合脚本撰写、配音合成、画面编排等技术。

5. **情感化交互设计**：应用不是冷冰冰的信息展示工具，而是能够理解学生心理、给予关怀和鼓励的智能伙伴。这要求对话系统具备情感计算能力，能够识别学生情绪并做出恰当回应。

6. **高中毕业生专属**：主要服务对象是即将升入大学的高中生，需要考虑他们对未来迷茫、对大学生活憧憬等特殊心理状态。

### 技术选型理由

#### 前端技术栈

| 技术 | 版本 | 用途 | 选择理由 |
|-----|------|------|---------|
| React | 18.x | Web端框架 | 生态成熟、组件化开发、社区活跃 |
| React Native | 0.72.x | 移动端框架 | 跨平台开发、代码复用率高 |
| Vue.js | 3.x | 小程序/轻量端 | 轻量级、学习成本低、适合快速迭代 |
| TypeScript | 5.x | 类型安全 | 减少运行时错误、提升代码可维护性 |
| Socket.IO | 4.x | 实时通信 | 支持双向通信、语音流传输 |
| Chart.js | 4.x | 图表库 | 轻量美观、文档完善 |
| Web Speech API | - | 语音识别/合成 | 浏览器原生支持、降级方案完善 |

#### 后端技术栈

| 技术 | 版本 | 用途 | 选择理由 |
|-----|------|------|---------|
| Python | 3.11 | 主要开发语言 | AI/ML生态丰富、爬虫库完善、开发效率高 |
| FastAPI | 0.109.x | Web框架 | 异步高性能、自动文档生成、类型提示完善 |
| Node.js | 20.x | 实时服务 | 事件驱动、适合高并发WebSocket连接 |
| LangChain | 0.1.x | AI应用框架 | 模块化设计、支持多种LLM、易于扩展 |
| Celery | 5.3.x | 异步任务队列 | 分布式任务调度、任务持久化支持 |
| Redis | 7.x | 缓存/消息队列 | 高性能、支持多种数据结构 |

#### AI与数据处理

| 技术 | 版本 | 用途 | 选择理由 |
|-----|------|------|---------|
| OpenAI GPT-4 | - | 对话模型 | 强大的语言理解和生成能力 |
| Whisper | - | 语音识别 | 多语言支持、准确率高 |
| ElevenLabs | - | 语音合成 | 自然度高、支持情感控制 |
| Scrapy | 2.11 | 爬虫框架 | 异步高效、扩展性强 |
| BeautifulSoup | 4.12 | HTML解析 | 使用简单、兼容性好 |
| Pandas | 2.x | 数据处理 | 功能强大、社区成熟 |
| NumPy | 1.26 | 数值计算 | 基础依赖、性能优异 |

#### 数据库与存储

| 技术 | 版本 | 用途 | 选择理由 |
|-----|------|------|---------|
| PostgreSQL | 15.x | 主数据库 | 功能完整、可靠性高、JSON支持 |
| Redis | 7.x | 缓存层 | 高性能、会话管理 |
| Elasticsearch | 8.x | 搜索引擎 | 全文检索、数据分析 |
| MinIO | - | 对象存储 | S3兼容、私有部署 |
| Chroma | - | 向量数据库 | 知识库检索、相似度匹配 |

#### 基础设施

| 技术 | 版本 | 用途 | 选择理由 |
|-----|------|------|---------|
| Docker | 24.x | 容器化 | 环境一致、部署便捷 |
| Docker Compose | 2.24.x | 容器编排 | 开发环境快速启动 |
| Nginx | 1.25 | 反向代理 | 性能优异、配置灵活 |
| Kafka | 3.6 | 消息队列 | 高吞吐、服务解耦 |
| Prometheus | - | 监控 | 指标采集、告警支持 |

### 架构设计理由

经过对项目需求的深入分析，我们决定采用**微服务架构**，理由如下：

1. **业务复杂度高**：应用包含多个相对独立的业务域（语音交互、信息爬取、推荐系统、可视化、视频生成、邮件服务等），每个业务域都有独特的技术栈和扩展需求，适合拆分为独立服务。

2. **团队协作需求**：项目涉及前端、后端、AI算法、数据工程、运维等多个专业领域，微服务架构可以让各团队独立开发和部署，提高协作效率。

3. **技术多样性**：项目需要用到Python（AI/爬虫）、Node.js（实时通信）、Go（高性能服务）等多种技术栈，微服务架构可以充分发挥各语言的优势。

4. **独立扩展能力**：不同服务的负载特性不同（如语音服务需要GPU资源，爬虫服务需要高并发IO），微服务架构允许对单个服务进行独立扩展。

5. **故障隔离**：某个服务的故障不会影响整体系统可用性，提高系统整体可靠性。

### 功能模块划分

系统按业务域划分为以下微服务：

```
专业选择指导应用
├── api-gateway/                    # API网关服务
│   ├── 路由分发
│   ├── 认证授权
│   ├── 限流熔断
│   └── 请求日志
├── user-service/                   # 用户服务
│   ├── 用户注册/登录
│   ├── 学生档案管理
│   ├── 偏好设置
│   └── 家长账户关联
├── voice-service/                  # 语音服务
│   ├── 语音识别（ASR）
│   ├── 语音合成（TTS）
│   ├── 语音预处理
│   └── 音频流处理
├── chat-service/                   # 对话服务
│   ├── 对话管理
│   ├── 意图识别
│   ├── 情感分析
│   └── 上下文维护
├── crawler-service/                # 爬虫服务
│   ├── 专业信息爬取
│   ├── 院校信息爬取
│   ├── 行业数据爬取
│   └── 反爬处理
├── major-service/                  # 专业服务
│   ├── 专业库管理
│   ├── 学科分类
│   ├── 课程信息
│   └── 专业对比
├── recommendation-service/         # 推荐服务
│   ├── 学生画像引擎
│   ├── 专业推荐引擎
│   ├── 院校匹配引擎
│   └── 推荐解释生成
├── analytics-service/              # 分析服务
│   ├── 专业趋势分析
│   ├── 就业前景分析
│   ├── 图表生成器
│   └── 报告生成器
├── video-service/                  # 视频服务
│   ├── 脚本生成器
│   ├── 配音合成器
│   ├── 画面编排器
│   └── 视频渲染器
├── email-service/                  # 邮件服务
│   ├── 邮件模板管理
│   ├── 邮件发送队列
│   ├── 发送记录管理
│   └── 退订管理
├── knowledge-base/                 # 知识库服务
│   ├── 专业知识图谱
│   ├── 院校知识库
│   ├── 职业发展路径
│   └── FAQ管理
└── notification-service/           # 通知服务
    ├── 消息队列管理
    ├── 多渠道推送
    ├── 消息模板管理
    └── 送达追踪
```

### 数据流设计

系统的核心数据流如下：

```
用户语音输入
    ↓
[voice-service] ASR语音识别 → 文本
    ↓
[api-gateway] 请求路由
    ↓
[chat-service] 意图识别 + 上下文理解
    ↓
    ├──→ [knowledge-base] 专业知识检索（如需）
    ├──→ [recommendation-service] 专业推荐（如需）
    ├──→ [analytics-service] 趋势分析（如需）
    └──→ [crawler-service] 专业信息爬取（如需）
    ↓
[chat-service] 生成回复 + 情感表达
    ↓
[voice-service] TTS语音合成
    ↓
用户语音输出
    ↓
（可选）[email-service] 发送专业报告到邮箱
```

---

## 整体规划

### 开发阶段规划

本项目采用敏捷开发模式，分为五个主要阶段：

#### 第一阶段：基础设施与核心框架（第1-2周）

**主要任务**：

1. 搭建微服务基础架构，完成所有服务的脚手架搭建
2. 部署Kubernetes集群（或Docker Compose开发环境）
3. 配置服务注册与发现、配置中心、日志系统
4. 建立API网关，完成基础路由和认证逻辑
5. 搭建数据库集群，完成数据模型设计
6. 搭建CI/CD流水线，实现自动化测试和部署

**交付物**：

- 微服务基础架构搭建完成
- API网关可正常工作
- 数据库初始化完成
- CI/CD流水线可用

#### 第二阶段：核心功能开发（第3-6周）

**主要任务**：

1. 语音服务开发（ASR、TTS集成）
2. 对话服务核心逻辑开发（意图识别、对话管理）
3. 用户服务开发（注册登录、学生档案）
4. 知识库服务搭建（专业知识图谱）
5. 前端多端框架搭建和基础组件开发

**交付物**：

- 语音对话功能可用
- 用户可以完成注册登录
- 基础对话功能可用
- Web端和移动端Demo可用

#### 第三阶段：业务功能完善（第7-10周）

**主要任务**：

1. 爬虫服务开发（专业信息、院校信息爬取）
2. 推荐算法实现（学生画像、专业匹配）
3. 数据分析模块开发（趋势分析、图表生成）
4. 视频生成服务开发
5. 邮件服务开发
6. 各端应用开发和完善

**交付物**：

- 专业信息爬取功能可用
- 专业推荐功能可用
- 数据分析和可视化功能可用
- 视频生成功能可用
- 邮件发送功能可用

#### 第四阶段：AI能力增强（第11-13周）

**主要任务**：

1. 大模型集成和提示词优化
2. 对话策略优化（心理引导、情感支持）
3. 推荐算法优化（多维度匹配）
4. 知识库扩充和优化
5. 视频生成质量优化

**交付物**：

- 智能助手对话质量显著提升
- 推荐准确率提高
- 视频内容质量优化

#### 第五阶段：测试优化与上线（第14-16周）

**主要任务**：

1. 全链路测试（单元测试、集成测试、压力测试）
2. 性能优化
3. 安全加固
4. 用户验收测试
5. 正式上线

**交付物**：

- 全功能可用的专业选择指导应用
- 完整的测试报告
- 运维文档和监控体系

### Agent分工

根据项目需求，我们设计以下Agent：

| Agent | 职责 | 技术栈 |
|-------|------|-------|
| **Coordinator** | 整体协调、任务拆分、进度管理、结果整合 | - |
| **Frontend** | 前端多端开发（Web、iOS、Android、小程序） | React、React Native、Vue |
| **Backend** | 后端服务开发（API网关、用户服务、对话服务） | Python、FastAPI |
| **AI/ML** | AI能力开发（对话引擎、推荐算法、语音处理） | Python、LangChain |
| **Data** | 数据服务开发（爬虫、分析、可视化） | Python、Scrapy |
| **DevOps** | 基础设施、容器化、CI/CD、监控 | Docker、K8s、Nginx |
| **DB-Expert** | 数据库设计、结构优化、数据建模 | PostgreSQL、Redis、Elasticsearch |
| **Prompt** | 架构建议、数据建模提示、需求分析引导 | - |
| **DocGenerator** | 文档生成、数据分析、开发执行 | - |
| **DocumentProcessor** | 文档爬取、分析、总结、生成专业介绍 | Python、BeautifulSoup、LLM |
| **VideoProcessor** | 视频搜索、总结、剪辑、生成讲解视频 | Python、FFmpeg、LLM |

### 新增Agent详细设计

#### DocumentProcessor Agent（文档处理Agent）

**职责描述：**
根据用户需求或特定信息，分析用户需求，爬取相关文本资料，进行智能总结，输出高质量的文档内容（如专业介绍、行业分析、职业前景等）。

**技术栈：**
- Python 3.11+
- BeautifulSoup4 / lxml（HTML解析）
- Playwright / Puppeteer（动态页面爬取）
- LangChain / LlamaIndex（文档处理和总结）
- OpenAI / 本地LLM（内容生成）
- Redis（缓存层）

**核心能力：**
1. **需求分析**：理解用户查询意图，提取关键信息（专业名称、类别、时间范围等）
2. **多源爬取**：从多个数据源爬取相关文本（官方网站、百科、论坛、新闻等）
3. **智能过滤**：去除广告、无关内容、低质量信息
4. **结构化总结**：将非结构化文本转换为结构化内容
5. **质量审核**：检查内容的准确性、时效性、完整性

#### VideoProcessor Agent（视频处理Agent）

**职责描述：**
搜索相关视频资源，对多个视频进行智能分析和总结，通过剪辑技术将内容压缩到合适时长，生成便于用户快速理解的视频摘要。

**技术栈：**
- Python 3.11+
- yt-dlp / you-get（视频下载）
- FFmpeg（视频处理和剪辑）
- OpenAI Whisper（语音识别）
- LangChain / LLM（内容总结）
- Redis（缓存层）

**核心能力：**
1. **视频搜索**：从B站、YouTube等平台搜索相关视频
2. **内容分析**：提取视频主题、关键知识点、时间戳
3. **语音识别**：将视频内容转换为文字（ASR）
4. **智能总结**：提取核心知识点和精华片段
5. **视频剪辑**：自动剪辑生成短视频摘要
6. **时长控制**：将视频压缩到合适时长（3-10分钟）

#### DB-Expert Agent（数据库专家Agent）

**职责描述：**
根据用户提出的业务需求，设计满足需求的数据表结构、索引策略和查询优化方案。支持关系型数据库（PostgreSQL、MySQL）和非关系型数据库（Redis、Elasticsearch、MongoDB）的设计。

**技术栈：**
- PostgreSQL 15.x（主数据库）
- Redis 7.x（缓存/会话）
- Elasticsearch 8.x（搜索引擎）
- MongoDB 4.x（文档存储）
- MySQL 8.x（可选）
- SQLAlchemy / Peewee（ORM）
- Docker（容器化部署）

**核心能力：**
1. **需求分析**：理解业务需求，提取实体、关系和约束条件
2. **关系型设计**：设计满足3NF的表结构、外键关系和索引
3. **非关系型设计**：设计适合特定场景的NoSQL数据模型
4. **查询优化**：分析慢查询，提供优化建议和索引策略
5. **数据迁移**：设计数据迁移方案和版本管理策略
6. **性能评估**：评估设计方案的读写性能和扩展性

#### Prompt Agent（提示引导Agent）

**职责描述：**
在项目讨论过程中，主动识别需要数据建模或架构设计的场景，向用户发出提示并引导完成数据模型设计。当用户提出需求或描述功能模块时，分析是否需要建立数据表、缓存模型、索引策略等，并在用户确认后自动调用DB-Expert完成设计。

**核心能力：**
1. **场景识别**：在对话中识别需要数据建模的场景（新增功能、数据存储、缓存需求、搜索需求等）
2. **智能提示**：向用户发出友好的数据建模建议，说明设计原因和价值
3. **需求引导**：引导用户描述数据需求，提取实体、关系和约束
4. **确认机制**：提供设计构思供用户确认，用户同意后才执行
5. **自动执行**：用户确认后，自动调用DB-Expert完成设计并更新文档
6. **文档同步**：将设计方案同步写入需求设计.md和相关文档

**工作流程：**
```
用户提出需求
    ↓
Prompt Agent识别需要数据建模
    ↓
向用户发出提示并提供设计构思
    ↓
用户确认（同意/不同意/修改）
    ↓
同意 → 自动调用DB-Expert执行设计
        → 更新需求设计.md
        → 创建相关Agent/Skill（如需要）
不同意 → 不执行，继续其他工作
```

**提示场景示例：**
- "我注意到这个功能需要存储用户数据，是否需要设计用户表？"
- "这个搜索功能可能需要Elasticsearch索引，我的构思是..."
- "这个缓存场景适合用Redis，我的建议是..."
- "这个报表需要聚合数据，需要设计统计表吗？"

#### DocGenerator Agent（文档生成Agent）

**职责描述：**
根据用户需求，分析项目中需要爬取数据的模块和业务，设计相应的数据表结构，并生成结构化的设计文档。生成文档后，询问用户是否开始开发，用户同意后可执行：
- 创建数据库表或数据模型
- 修改后端代码（API接口、数据访问层）
- 修改前端代码（页面组件、API调用）
- 确保项目正常运行

**核心能力：**
1. **需求分析**：分析项目中需要爬取数据的模块和业务，识别数据来源、更新频率和数据量级
2. **数据模型设计**：为不同业务设计合适的数据表结构、索引策略和关联关系
3. **文档生成**：生成包含DDL脚本、ER图、数据字典的结构化设计文档
4. **确认机制**：生成文件前和开发执行前都需要获得用户确认
5. **开发执行**：用户同意后可执行代码开发，可调用Backend/Frontend/DB-Expert Agent辅助

**可调用的Agent：**
- **DB-Expert**：辅助设计数据库表结构和索引
- **Prompt**：识别潜在的数据建模需求
- **Backend**：辅助开发后端API和数据访问层
- **Frontend**：辅助开发前端页面和组件

**工作流程：**
```
用户：帮我生成XX分析文档
    ↓
分析爬虫数据需求（doc-generator-analyze）
    ↓
设计数据模型（doc-generator-design，可调用DB-Expert）
    ↓
向用户确认（doc-generator-confirm）
    ↓
用户确认 → 生成文档到 doc/ 目录（doc-generator-generate）
    ↓
询问是否开始开发
    ↓
用户同意 → 执行开发（doc-generator-execute）
           ├── 创建数据库表
           ├── 修改后端代码
           ├── 修改前端代码
           └── 验证项目运行
```

**使用示例：**
- "帮我生成爬虫数据分析文档"
- "帮我设计用户收藏功能的数据模型并生成文档"
- "根据这份设计文档，开始开发"

### Skill调度

各Agent包含以下核心Skill：

| Agent | Skill | 功能 |
|-------|-------|------|
| Coordinator | coordinator-analyze | 需求分析和任务拆分 |
| Coordinator | coordinator-plan | 整体规划和进度管理 |
| Coordinator | coordinator-integrate | 结果整合和质量把控 |
| Frontend | frontend-analyze | 前端需求分析 |
| Frontend | frontend-plan | 前端技术选型和架构设计 |
| Frontend | frontend-web | Web端开发 |
| Frontend | frontend-mobile | 移动端开发 |
| Frontend | frontend-miniprogram | 小程序开发 |
| Backend | backend-analyze | 后端需求分析 |
| Backend | backend-plan | 后端架构设计 |
| Backend | backend-api | API开发 |
| Backend | backend-gateway | 网关开发 |
| AI/ML | aianalyze | AI需求分析 |
| AI/ML | ai-plan | AI架构设计 |
| AI/ML | ai-chat | 对话引擎开发 |
| AI/ML | ai-voice | 语音处理开发 |
| AI/ML | ai-recommend | 推荐算法开发 |
| Data | data-analyze | 数据需求分析 |
| Data | data-plan | 数据架构设计 |
| Data | data-crawler | 爬虫开发 |
| Data | data-analytics | 数据分析开发 |
| Data | data-visualize | 可视化开发 |
| DevOps | devops-analyze | 运维需求分析 |
| DevOps | devops-plan | 基础设施规划 |
| DevOps | devops-docker | Docker配置 |
| DevOps | devops-k8s | K8s集群配置 |
| DevOps | devops-ci | CI/CD流水线 |
| DocumentProcessor | doc-analyze | 文档需求分析和意图理解 |
| DocumentProcessor | doc-crawler | 多源文本爬取 |
| DocumentProcessor | doc-filter | 智能内容过滤 |
| DocumentProcessor | doc-summarize | 文档智能总结 |
| DocumentProcessor | doc-generate | 专业介绍生成 |
| VideoProcessor | video-search | 多平台视频搜索 |
| VideoProcessor | video-download | 视频下载和转码 |
| VideoProcessor | video-asr | 语音识别（ASR） |
| VideoProcessor | video-analyze | 视频内容分析 |
| VideoProcessor | video-clip | 智能视频剪辑 |
| VideoProcessor | video-summary | 视频摘要生成 |
| DB-Expert | db-expert-analyze | 数据库需求分析 |
| DB-Expert | db-expert-design | 数据库结构设计 |
| DB-Expert | db-expert-optimize | 查询优化和索引策略 |
| DB-Expert | db-expert-nosql | NoSQL数据模型设计 |
| DB-Expert | db-expert-migrate | 数据迁移和版本管理 |
| Prompt | prompt-identify | 数据建模场景识别 |
| Prompt | prompt-suggest | 智能建模建议 |
| Prompt | prompt-confirm | 需求确认和引导 |
| Prompt | prompt-execute | 自动执行设计 |
| DocGenerator | doc-generator-analyze | 爬虫数据需求分析 |
| DocGenerator | doc-generator-design | 数据模型设计 |
| DocGenerator | doc-generator-confirm | 设计确认和交互 |
| DocGenerator | doc-generator-generate | 文档生成技能 |
| DocGenerator | doc-generator-execute | 执行开发任务 |

### 里程碑交付物

| 阶段 | 里程碑 | 交付物 |
|-----|--------|-------|
| 第1周 | 基础设施就绪 | 架构文档、代码仓库、CI/CD流水线 |
| 第2周 | 核心框架完成 | API网关、用户服务、数据库初始化 |
| 第4周 | 核心功能可用 | 语音对话、用户注册、Web端Demo |
| 第6周 | 核心功能完善 | 全部基础功能、移动端Demo |
| 第8周 | 业务功能完成 | 爬虫、推荐、分析功能 |
| 第10周 | 高级功能完成 | 视频生成、邮件服务 |
| 第12周 | AI能力增强 | 优化后的对话和推荐 |
| 第14周 | 测试完成 | 测试报告、性能报告 |
| 第16周 | 正式上线 | 生产环境部署、运维文档 |

---

## 信息查询与总结规范

本项目针对任意学科的信息查询、总结、视频搜索和剪辑制定统一规范，所有Agent和Skill必须遵循以下规则。

### 一、文本信息查询规则

#### 1.1 查询流程

```
用户查询请求
    ↓
意图识别（识别专业名称、查询类型）
    ↓
多源数据爬取（官方网站、百科、教育网站）
    ↓
内容质量过滤（去广告、去无关内容）
    ↓
结构化处理（提取关键信息）
    ↓
时间线格式化输出
```

#### 1.2 查询类型识别

| 查询类型 | 识别关键词 | 处理方式 |
|---------|-----------|---------|
| 专业介绍 | "是什么"、"介绍"、"定义" | 返回时间线格式的专业发展历程 |
| 课程信息 | "学什么"、"课程"、"专业课" | 返回核心课程列表和学习路径 |
| 就业前景 | "就业"、"工作"、"薪资" | 返回就业方向和薪资水平 |
| 报考信息 | "分数线"、"录取"、"多少分" | 返回历年录取数据 |
| 大学推荐 | "大学"、"学校"、"报考" | 返回推荐院校列表 |

#### 1.3 数据源优先级

| 优先级 | 数据源 | 说明 |
|-------|--------|------|
| 1 | 阳光高考 | 官方权威数据 |
| 2 | 各高校官网 | 最准确的招生信息 |
| 3 | 中国教育在线 | 专业介绍和排名 |
| 4 | 知乎/豆瓣 | 经验分享和评价 |
| 5 | B站/小红书 | 真实学习体验 |

#### 1.4 内容质量标准

| 标准 | 要求 |
|-----|------|
| 时效性 | 数据必须来自近3年的权威来源 |
| 准确性 | 关键数据（分数线、薪资）需多源验证 |
| 完整性 | 覆盖起源、发展、现状、未来四个维度 |
| 可读性 | 语言通俗易懂，避免过度专业化表述 |

### 二、专业介绍总结规则

#### 2.1 时间线格式规范

所有专业介绍必须采用**时间线叙述格式**，包含以下5个核心部分：

```
**起源与发展**  
[专业名称]的源头可追溯至[具体时间]的[具体事件/人物]。[关键里程碑]标志着[专业]进入[新阶段]。

**挫折与争议**  
[专业]发展历程中经历过[具体挫折/争议]。[影响]。[应对措施]。

**重大突破**  
[年份]，[具体事件]是[专业]史上最伟大的[突破/成就]。[后续影响]。

**现状与爆发**  
当前，[专业]正处于[发展态势]。[具体数据/案例]。[代表性成就]。

**未来展望**  
[专业]未来将在[方向]取得[预期成就]。[人才需求]。[职业前景]。
```

#### 2.2 热点事件整合

在"现状与爆发"部分，应整合最新的热点事件：

| 热点类型 | 示例 |
|---------|------|
| 技术突破 | "2024年，OpenAI发布GPT-4o，引发行业轰动" |
| 行业动态 | "2024年，某AI公司上市，市值突破1000亿" |
| 政策变化 | "2024年，国家发布人工智能发展规划" |
| 重要事件 | "2024年，世界人工智能大会在上海召开" |

```python
def get_hot_events_for_major(major_name: str) -> List[Dict]:
    """获取专业的热点事件"""
    # 从知乎、B站、新闻等平台获取
    hot_events = []
    
    # 搜索近期热点
    keywords = [
        f"{major_name}最新消息",
        f"{major_name}热点",
        f"{major_name}新闻 2024",
        f"{major_name}突破"
    ]
    
    for keyword in keywords:
        events = search_hot_events(keyword)
        hot_events.extend(events)
    
    # 按热度排序，去重
    hot_events = deduplicate_events(hot_events)
    hot_events.sort(key=lambda x: x.get("heat_index", 0), reverse=True)
    
    return hot_events[:5]  # 返回前5个热点
```

#### 2.3 内容要素要求

| 要素 | 必须包含 | 示例 |
|-----|---------|------|
| 具体时间 | ✓ | "1956年"、"19世纪末" |
| 关键人物 | ✓ | "约翰·麦卡锡"、"亚当·斯密" |
| 重要事件 | ✓ | "达特茅斯会议"、"《国富论》发表" |
| 国家/地区 | ✓ | "美国"、"欧洲"、"中国" |
| 数据支撑 | ✓ | "市场规模5000亿"、"就业率95%" |

#### 2.3 专业分类模板

| 学科门类 | 侧重点 | 特有内容 |
|---------|-------|---------|
| 工学 | 技术发展 | 关键技术突破、产业应用 |
| 理学 | 理论创新 | 基础理论、科学发现 |
| 医学 | 技术进步 | 重大疗法、医疗技术 |
| 法学 | 制度演变 | 法律制定、重大案例 |
| 经济学 | 市场变化 | 经济理论、市场发展 |
| 文学 | 文化传承 | 代表作品、文化运动 |
| 管理学 | 方法创新 | 管理理论、企业案例 |

#### 2.4 热门专业检测

```python
# 热门专业判断规则
HOT_MAJORS = {
    # AI相关
    "人工智能", "机器学习", "深度学习", "自然语言处理", "计算机视觉",
    # 新兴技术
    "数据科学与大数据技术", "云计算", "物联网", "区块链", "网络安全",
    # 热门工程
    "计算机科学与技术", "软件工程", "电子信息工程", "自动化",
    # 热门经济管理
    "金融学", "经济学", "工商管理",
    # 热门医学
    "临床医学", "口腔医学",
    # 热门法学
    "法学"
}

def is_hot_major(major_name: str) -> bool:
    """判断是否为热门学科"""
    for hot in HOT_MAJORS:
        if hot in major_name or major_name in hot:
            return True
    return False
```

#### 2.5 缓存策略

| 专业类型 | 缓存时间 | 说明 |
|---------|---------|------|
| 热门专业 | 24小时 | AI、金融、计算机等 |
| 普通专业 | 72小时 | 其他专业 |
| 实时数据 | 无缓存 | 分数线、录取数据 |

### 三、视频搜索规则

#### 3.1 搜索关键词策略

针对不同专业，自动构建多组搜索关键词：

```python
def build_search_keywords(major_name: str) -> List[str]:
    """构建专业视频搜索关键词"""
    return [
        f"{major_name}专业介绍",      # 专业解读
        f"{major_name}专业解读",       # 深度解读
        f"什么是{major_name}",         # 概念介绍
        f"{major_name}就业前景",       # 就业分析
        f"{major_name}报考指南",       // 报考指导
        f"{major_name}学长学姐",       // 经验分享
    ]
```

#### 3.2 搜索平台

| 平台 | 优先级 | 说明 |
|-----|-------|------|
| B站 | 1 | 国内最大学习视频平台，内容丰富 |
| 知乎 | 2 | 专业问答和经验分享 |
| 小红书 | 3 | 真实学习体验分享 |

#### 3.3 视频筛选标准

| 标准 | 要求 |
|-----|------|
| **视频数量** | **只返回1个最佳视频** |
| **内容覆盖** | 必须覆盖时间线5个阶段：起源与发展、挫折与争议、重大突破、现状与爆发、未来展望 |
| 时长 | 优先3-5分钟的短视频（<300秒最佳） |
| 播放量 | 优先1000+播放的视频 |
| 发布时间 | 优先近2年的视频 |
| 内容质量 | 优先有完整知识体系的视频 |
| 相关度 | 标题和描述必须包含专业名称关键词 |

#### 3.4 视频评分规则

```python
def calculate_video_score(video: Dict, major_name: str) -> float:
    """计算视频评分，返回最高分的视频"""
    score = 0.0
    
    # 内容覆盖度（最重要，权重50%）
    title = video.get("title", "")
    description = video.get("description", "")
    content = f"{title} {description}".lower()
    
    required_keywords = [
        "起源", "发展", "历史",      # 起源与发展
        "挫折", "争议", "低谷",      # 挫折与争议
        "突破", "成就", "里程碑",    # 重大突破
        "现状", "爆发", "现在",      # 现状与爆发
        "未来", "展望", "趋势"       # 未来展望
    ]
    
    coverage_count = sum(1 for kw in required_keywords if kw in content)
    coverage_score = (coverage_count / 5) * 50  # 满分50分
    
    # 时长评分（权重20%），最佳3-5分钟
    duration = video.get("duration", 0)
    if duration <= 60:
        duration_score = 10  # 太短
    elif duration <= 300:
        duration_score = 20  # 最佳
    elif duration <= 600:
        duration_score = 15  # 可接受
    else:
        duration_score = 5   # 太长
    
    # 播放量评分（权重15%）
    view_count = video.get("view_count", 0)
    if view_count >= 10000:
        view_score = 15
    elif view_count >= 5000:
        view_score = 12
    elif view_count >= 1000:
        view_score = 10
    else:
        view_score = 5
    
    # 相关度评分（权重15%）
    relevance_score = 0
    if major_name in title:
        relevance_score += 10
    if "专业" in title or "介绍" in title or "解读" in title:
        relevance_score += 5
    
    total_score = coverage_score + duration_score + view_score + relevance_score
    return total_score
```

#### 3.5 B站API调用规范

```python
# B站搜索API调用规范
BILIBILI_API = "https://api.bilibili.com/x/web-interface/search/type"

# 必需请求头
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Referer": "https://www.bilibili.com"
}

# 搜索参数
SEARCH_PARAMS = {
    "search_type": "video",
    "keyword": keyword,           # 搜索关键词
    "page": 1,                   # 页码
    "page_size": 10,             # 每页数量
    "order": "totalrank"         # 排序方式
}

# 响应解析
VIDEO_FIELDS = [
    "bvid",          # B站视频ID
    "title",         # 标题
    "description",   # 描述
    "pic",           # 封面图
    "duration",      # 时长
    "author",        # 作者
    "play",          # 播放量
    "pubdate",       # 发布时间
]
```

#### 3.6 热点事件获取规则

##### 3.6.1 热点事件定义

热点事件是指**与专业相关的最新重大事件或新闻**，包括但不限于：

| 事件类型 | 示例 |
|---------|------|
| 技术突破 | "OpenAI发布GPT-5"、"特斯拉发布自动驾驶新算法" |
| 行业动态 | "某AI公司上市"、"某技术获得诺贝尔奖" |
| 政策变化 | "国家发布AI发展规划"、"某专业列入强基计划" |
| 重要会议 | "世界AI大会召开"、"诺奖得主来华讲座" |
| 社会事件 | "AI换脸引发争议"、"算法歧视事件" |

##### 3.6.2 热点事件数据源

| 数据源 | 类型 | 优先级 | 说明 |
|-------|------|--------|------|
| **B站** | 视频+文章 | 1 | 视频热点、科普内容（带播放量、时长，按最新发布时间排序） |
| **微博** | 热搜 | 2 | 社会热点和舆论（带讨论量） |
| **今日头条** | 新闻 | 3 | 综合新闻资讯（带阅读量） |
| **腾讯新闻** | 新闻 | 4 | 权威新闻报道（带评论量） |

##### 3.6.3 热点事件搜索关键词

```python
def build_hot_event_keywords(major_name: str) -> List[str]:
    """构建热点事件搜索关键词（包含行业大佬和关键术语）"""
    base_keywords = [
        f"{major_name} 2025",           # 最新事件
        f"{major_name} 最新",            # 最新消息
        f"{major_name} 热点",            # 热点事件
        f"{major_name} 重大突破",        # 重大突破
        f"{major_name} GPT",             # GPT相关
        f"{major_name} DeepSeek",        # DeepSeek相关
        f"{major_name} Agent",           # Agent相关
        f"AI {major_name} 2025"          # AI相关
    ]
    
    # 行业大佬关键词（重点关注领域大佬动态）
    leader_keywords = [
        f"{major_name} 黄仁勋",          # 英伟达CEO
        f"{major_name} 马斯克",          # 特斯拉/SpaceX CEO
        f"{major_name} Agent Skill",     # Agent技能
        f"{major_name} 估值",            # 公司融资估值动态
        f"{major_name} OpenAI",          # OpenAI相关
        f"{major_name} 英伟达",          # 英伟达相关
        f"{major_name} 月之暗面"         # 国内AI公司动态
    ]
    
    return base_keywords + leader_keywords
```

##### 3.6.4 热点事件筛选标准

| 标准 | 要求 |
|-----|------|
| **时间范围** | 最近**180天**内的事件（6个月） |
| **排序方式** | 按时间倒序（最新在前）+ 热度加权 |
| **相关度** | 必须与专业直接相关 |
| **重要性** | 优先重大事件（突破、政策、行业变化、大佬动态） |
| **权威性** | 优先权威媒体报道 |
| **数量限制** | 最多返回15个热点事件 |

##### 3.6.5 内容类型规则

**核心原则：热点内容不一定是视频，也可以是跳转到外部网页链接**

| 内容类型 | 说明 | 优先级 |
|---------|------|-------|
| **视频内容** | B站、YouTube等平台的视频 | 优先（带封面、时长、播放量） |
| **新闻资讯** | 微博、今日头条、腾讯新闻等平台的文章 | 优先（带阅读量、来源） |
| **社交动态** | 微博热搜、小红书的热门讨论 | 次优（带讨论热度） |

##### 3.6.6 行业大佬动态规则

**核心原则：查找热点时，多关注领域大佬的动态和新闻**

| 专业领域 | 行业大佬/公司 | 搜索关键词 |
|---------|-------------|-----------|
| **人工智能** | 黄仁勋（Jensen Huang）、马斯克（Elon Musk） | "人工智能 黄仁勋"、"人工智能 马斯克" |
| | OpenAI、英伟达（NVIDIA） | "人工智能 OpenAI"、"人工智能 英伟达" |
| | 月之暗面（MoonShot AI） | "人工智能 估值"、"AI Agent Skill" |
| **计算机科学** | 比尔·盖茨、扎克伯格 | "计算机 微软"、"Meta 最新" |
| **软件工程** | 马斯克、扎克伯格 | "软件 工程 特斯拉"、"代码 开源" |
| **自动驾驶/新能源** | 马斯克、李想、何小鹏 | "自动驾驶 特斯拉"、"新能源 车" |
| **生物医药** | 各大药企CEO | "医药 突破"、"创新药 研发" |

##### 3.6.7 180天时间过滤规则

```python
from datetime import datetime, timedelta

def filter_events_by_days(events: List[Dict], days: int = 180) -> List[Dict]:
    """过滤保留最近指定天数内的事件"""
    now = datetime.now()
    cutoff = timedelta(days=days)
    filtered = []
    
    for event in events:
        pub_date = event.get("pub_date", "")
        if pub_date:
            try:
                event_date = datetime.strptime(pub_date, "%Y-%m-%d")
                if (now - event_date) <= cutoff:
                    filtered.append(event)
            except:
                # 如果日期解析失败，保留该事件
                filtered.append(event)
        else:
            # 没有日期信息的事件也保留
            filtered.append(event)
    
    return filtered

def sort_events_by_time_and_heat(events: List[Dict]) -> List[Dict]:
    """按时间倒序+热度加权排序"""
    return sorted(events, key=lambda x: (x.get("pub_date", ""), x.get("heat_index", 0)), reverse=True)
```

##### 3.6.8 政治敏感内容过滤规则

**严格禁止返回涉及政治敏感内容的热点事件**

| 敏感类型 | 示例 | 处理方式 |
|---------|------|---------|
| 政治领导人 | 涉及国家领导人姓名、绰号、负面信息 | 直接过滤 |
| 抗议活动 | 游行示威、罢工罢课、维权事件 | 直接过滤 |
| 宗教民族 | 邪教组织、民族分裂、宗教冲突 | 直接过滤 |
| 历史事件 | 敏感历史时期、事件、人物 | 直接过滤 |
| 领土主权 | 领土争议、边界问题、分离活动 | 直接过滤 |
| 邪教组织 | 法轮功、全能神等邪教内容 | 直接过滤 |

**过滤算法：**

```python
SENSITIVE_KEYWORDS = [
    # 政治相关
    "领导人", "主席", "总理", "总统", "习近平", "李克强", "温家宝", "胡锦涛",
    "示威", "抗议", "罢工", "罢课", "维权", "上访",
    "法轮功", "全能神", "观音法门", "呼喊派", "门徒会", "血水圣灵",
    "藏独", "疆独", "台独", "港独", "分裂", "独立",
    # 历史敏感
    "文革", "六四", "天安门", "坦克人",
]

def is_sensitive_content(title: str, description: str) -> bool:
    """检查内容是否涉及敏感话题"""
    content = f"{title} {description}".lower()
    for keyword in SENSITIVE_KEYWORDS:
        if keyword.lower() in content:
            return True
    return False

def filter_hot_content(items: List[Dict]) -> List[Dict]:
    """过滤热点内容中的敏感信息"""
    filtered = []
    for item in items:
        title = item.get("title", "")
        description = item.get("description", "")
        if not is_sensitive_content(title, description):
            filtered.append(item)
    return filtered
```

**使用示例：**

```python
# 获取热点事件后过滤敏感内容
hot_events = get_hot_events(major_name)
safe_events = filter_hot_content(hot_events)
```

##### 3.6.7 热点内容数据结构

视频内容应尽可能包含最新的热点事件，评分时应考虑视频是否涉及近期热点：

```python
def calculate_video_score(video: Dict, major_name: str, hot_events: List[Dict]) -> float:
    """计算视频评分，加入热点事件因素"""
    base_score = 0.0
    
    # ... 原有评分逻辑 ...
    
    # 热点事件加成（权重10%）
    hot_event_bonus = 0
    title = video.get("title", "").lower()
    description = video.get("description", "").lower()
    content = f"{title} {description}"
    
    for event in hot_events[:3]:  # 只考虑前3个热点
        event_title = event.get("title", "").lower()
        # 如果视频标题包含热点事件关键词，加分
        if any(kw in content for kw in event_title.split()[:5]):
            hot_event_bonus += 3  # 每个相关热点加3分
    
    total_score = base_score + min(hot_event_bonus, 10)  # 最多加10分
    return total_score
```

#### 3.7 搜索结果去重规则

| 去重级别 | 规则 | 实现方式 |
|---------|------|---------|
| 完全去重 | 相同BV号视为同一视频 | Set集合 |
| 近似去重 | 标题相似度>80%视为重复 | SimHash |
| 系列去重 | 同一UP主的系列视频归类 | UP主+标题匹配 |

### 四、视频总结规则

#### 4.1 总结内容要求

| 内容类型 | 说明 | 长度 |
|---------|------|------|
| 摘要文本 | 视频核心内容的连贯叙述 | 300-500字 |
| 关键知识点 | 3-5个最重要的知识点 | 每条50字内 |
| 时间戳 | 3-5个重要时间点 | 格式：00:00-说明 |
| 剪辑脚本 | 需要剪辑的片段列表 | 3-5个片段 |

#### 4.2 摘要写作规范

```markdown
# 视频摘要写作规则

## ✅ 正确做法
- 写成连贯的叙述性文字
- 重点是让用户通过文字了解视频核心内容
- 保持逻辑连贯性

## ❌ 错误做法
- 简单列出关键点
- 碎片化信息堆砌
- 缺少上下文连接
```

#### 4.3 时间戳标注规则

| 规则 | 说明 |
|-----|------|
| 标注数量 | 3-5个最重要的时间点 |
| 格式 | "时间 - 说明" |
| 选择标准 | 知识点切换、核心概念讲解 |

示例：
```
0:00 - 开场介绍
3:15 - 核心概念讲解
8:30 - 案例分析
12:45 - 应用场景
18:00 - 总结展望
```

#### 4.4 关键点提取规则

| 规则 | 说明 |
|-----|------|
| 数量 | 3-5个最重要知识点 |
| 表达 | 简洁、清晰 |
| 逻辑 | 知识点之间有逻辑关系 |

### 五、视频剪辑规则

#### 5.1 剪辑目标

| 目标 | 说明 |
|-----|------|
| 时长控制 | 最终输出3-5分钟 |
| 内容精华 | 保留核心知识点 |
| 逻辑完整 | 保持内容连贯性 |

#### 5.2 剪辑片段选择标准

| 标准 | 要求 |
|-----|------|
| 知识点密度 | 选择信息量大的片段 |
| 讲解质量 | 选择讲解清晰的部分 |
| 技术可行性 | 确保片段边界清晰 |
| 内容完整性 | 覆盖主要知识点 |

#### 5.3 剪辑脚本格式

```json
{
  "clips": [
    {
      "start": 0,
      "end": 195,
      "content": "开场介绍视频背景和主题",
      "label": "开场"
    },
    {
      "start": 510,
      "end": 765,
      "content": "核心概念详细讲解",
      "label": "核心内容"
    }
  ],
  "total_duration": 1200,
  "output_duration": 300
}
```

### 六、输出格式规范

#### 6.1 专业介绍输出格式

```json
{
  "success": true,
  "major_name": "人工智能",
  "category": "工学",
  "introduction": "**起源与发展**  \n1956年夏天...",
  "core_courses": ["人工智能导论", "机器学习", "深度学习"],
  "career_prospects": "人工智能专业毕业生就业前景广阔...",
  "related_majors": ["计算机科学与技术", "数据科学与大数据技术"],
  "is_hot_major": true,
  "cache_ttl_hours": 24,
  "generated_at": "2026-01-22T22:00:00"
}
```

#### 6.2 视频搜索输出格式

```json
{
  "major_name": "人工智能",
  "total_results": 5,
  "page": 1,
  "page_size": 5,
  "videos": [
    {
      "bvid": "BV1xxx",
      "title": "人工智能专业介绍",
      "description": "...",
      "cover": "https://...",
      "duration": 600,
      "author": "UP主名",
      "view_count": 10000,
      "pubdate": 1700000000,
      "url": "https://www.bilibili.com/video/BV1xxx"
    }
  ]
}
```

#### 6.3 视频摘要输出格式

```json
{
  "video": {
    "bvid": "BV1xxx",
    "title": "人工智能专业介绍",
    "duration": 1200
  },
  "summary": "本视频主要介绍人工智能专业的...",
  "key_points": [
    "了解人工智能的基本概念",
    "掌握机器学习核心算法",
    "学习深度学习应用"
  ],
  "timestamps": [
    {"time": 0, "label": "开场"},
    {"time": 180, "label": "核心概念"},
    {"time": 450, "label": "案例分析"}
  ],
  "generated_at": "2026-01-22T22:00:00"
}
```

### 七、质量保证机制

#### 7.1 内容验证

| 验证项 | 检查方式 |
|-------|---------|
| 数据真实性 | 多源交叉验证 |
| 时效性 | 检查数据更新时间 |
| 完整性 | 验证5个时间线部分 |
| 可读性 | 语言清晰度检查 |

#### 7.2 错误处理

| 错误类型 | 处理方式 |
|---------|---------|
| 数据源不可用 | 使用缓存数据或返回友好提示 |
| 格式错误 | 返回错误信息，保留部分数据 |
| 超时 | 返回部分结果或超时提示 |
| 无结果 | 返回空结果数组，提供搜索建议 |

#### 7.3 性能要求

| 操作 | 响应时间 | 说明 |
|-----|---------|------|
| 专业介绍查询 | < 2秒 | 含缓存 |
| 视频搜索 | < 3秒 | B站API调用 |
| 视频摘要 | < 10秒 | 含字幕分析 |
| 缓存命中 | < 100ms | 纯缓存读取 |

### 八、相关Skill定义

#### 8.1 DocumentProcessor Agent Skills

| Skill名称 | 功能 | 触发词 |
|----------|------|-------|
| doc_crawler | 文本信息爬取 | 爬取、搜索、查找 |
| doc_analyze | 内容分析处理 | 分析、理解、提取 |
| doc_summarize | 专业介绍生成 | 介绍、总结、概述 |
| doc_generate | 内容生成输出 | 生成、输出、导出 |

#### 8.2 VideoProcessor Agent Skills

| Skill名称 | 功能 | 触发词 |
|----------|------|-------|
| video_search | 视频搜索查找 | 搜索、查找、找视频 |
| video_asr | 语音识别转文字 | 转文字、字幕、识别 |
| video_analyze | 视频内容分析 | 分析、理解、内容 |
| video_summary | 视频摘要生成 | 总结、摘要、概括 |
| video_clip | 视频剪辑处理 | 剪辑、裁剪、片段 |

---

### 容器化策略

本项目采用完全容器化部署策略，所有服务均通过Docker运行：

| 服务类型 | 容器化策略 | 理由 |
|---------|-----------|------|
| 应用服务 | 全部容器化 | 统一部署、便于扩展 |
| 数据库 | 容器化（生产环境可裸机） | 便于管理、数据持久化 |
| 消息队列 | 容器化 | 服务解耦、状态无关 |
| 搜索引擎 | 容器化 | 便于集群管理 |
| 缓存 | 容器化 | 便于扩展 |
| 监控 | 容器化 | 便于部署和维护 |

### 外部服务集成

项目使用以下外部Docker镜像：

| 服务 | 镜像 | 端口 | 说明 |
|-----|------|------|------|
| PostgreSQL | postgres:15-alpine | 5432 | 主数据库 |
| Redis | redis:7-alpine | 6379 | 缓存和会话 |
| Elasticsearch | elasticsearch:8.10 | 9200/9300 | 搜索引擎 |
| MinIO | minio/minio | 9000/9001 | 对象存储 |
| Kafka | confluentinc/cp-kafka:7.4 | 9092 | 消息队列 |
| Zookeeper | confluentinc/cp-zookeeper:7.4 | 2181 | Kafka依赖 |
| Nginx | nginx:alpine | 80/443 | 反向代理 |
| Prometheus | prom/prometheus:v2.47 | 9090 | 监控 |
| Grafana | grafana/grafana:10.2 | 3000 | 可视化面板 |

### 数据持久化方案

```yaml
volumes:
  # 数据库数据
  postgres-data:
    driver: local
  redis-data:
    driver: local
  elasticsearch-data:
    driver: local
  
  # 对象存储
  minio-data:
    driver: local
  
  # 消息队列数据
  kafka-data:
    driver: local
  zookeeper-data:
    driver: local
  
  # 应用数据
  app-uploads:
    driver: local
  app-logs:
    driver: local
  
  # 监控数据
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
```

### 网络配置

```yaml
networks:
  app-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
  
  microservices-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.29.0.0/16
    
  monitoring-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/16
```

---

## 项目启动流程

### 启动步骤

#### 步骤一：环境准备

```bash
# 1. 确保已安装Docker和Docker Compose
docker --version    # Docker version 24.0+
docker-compose --version  # Docker Compose version 2.24+

# 2. 克隆代码仓库
git clone <repository-url>
cd major-guidance-app

# 3. 创建环境变量文件
cp .env.example .env
# 编辑.env文件，配置必要的环境变量
```

#### 步骤二：启动基础服务

```bash
# 启动网络和存储层
docker network create app-network || true
docker network create microservices-network || true
docker network create monitoring-network || true

# 启动数据存储服务
docker-compose -f docker-compose.infra.yml up -d

# 验证服务状态
docker-compose -f docker-compose.infra.yml ps
```

#### 步骤三：启动应用服务

```bash
# 启动所有微服务
docker-compose -f docker-compose.yml up -d

# 查看服务状态
docker-compose ps

# 查看启动日志
docker-compose logs -f
```

### 服务启动顺序

```
1. 网络层
   ├── 创建app-network
   ├── 创建microservices-network
   └── 创建monitoring-network

2. 存储层
   ├── PostgreSQL（等待约10秒）
   ├── Redis
   ├── Elasticsearch
   ├── MinIO
   └── 等待所有存储服务健康

3. 消息队列层
   ├── Zookeeper
   └── Kafka

4. 基础设施层
   ├── Nginx
   └── 监控服务（Prometheus、Grafana）

5. 核心服务层
   ├── api-gateway
   ├── user-service
   └── 等待核心服务启动

6. 业务服务层
   ├── voice-service
   ├── chat-service
   ├── crawler-service
   ├── major-service
   ├── recommendation-service
   └── 等待业务服务启动

7. 增强服务层
   ├── analytics-service
   ├── video-service
   ├── email-service
   └── knowledge-base

8. 前端层
   ├── web-frontend
   └── 编译静态资源
```

### 健康检查

```bash
# 检查所有服务健康状态
curl -s http://localhost:80/health | jq

# 单独检查各服务
curl -s http://localhost:8000/health  # api-gateway
curl -s http://localhost:8001/health  # user-service
curl -s http://localhost:8002/health  # voice-service
curl -s http://localhost:8003/health  # chat-service

# 检查数据库连接
docker-compose exec -T postgres pg_isready -U postgres

# 检查Redis连接
docker-compose exec redis redis-cli ping
```

### 故障排查

```bash
# 查看服务日志
docker-compose logs -f [service-name]

# 查看最近的错误日志
docker-compose logs --tail=100 [service-name] | grep -i error

# 进入服务容器排查
docker-compose exec [service-name] /bin/bash

# 检查容器资源使用
docker stats

# 检查网络连通性
docker network inspect app-network
```

---

## Docker Compose命令

### 一键启动

```bash
# 启动所有服务（开发环境）
docker-compose up -d

# 启动所有服务并重新构建镜像
docker-compose up -d --build

# 启动基础服务
docker-compose -f docker-compose.infra.yml up -d

# 启动应用服务
docker-compose -f docker-compose.app.yml up -d
```

### 一键停止

```bash
# 停止所有服务
docker-compose down

# 停止并删除数据卷（谨慎使用，数据将丢失）
docker-compose down -v

# 停止并删除网络
docker-compose down --remove-orphans
```

### 重启服务

```bash
# 重启所有服务
docker-compose restart

# 重启特定服务
docker-compose restart [service-name]

# 重启并重新构建
docker-compose up -d --no-deps [service-name]
```

### 查看状态和日志

```bash
# 查看服务状态
docker-compose ps

# 查看实时日志
docker-compose logs -f

# 查看特定服务日志
docker-compose logs -f [service-name]

# 查看最近的100行日志
docker-compose logs --tail=100
```

### 数据管理

```bash
# 备份数据库
docker-compose exec -T postgres pg_dump -U postgres employment > backup.sql

# 恢复数据库
cat backup.sql | docker-compose exec -T postgres psql -U postgres employment

# 备份Redis数据
docker-compose exec redis redis-cli BGSAVE
docker-compose cp redis:/data/dump.rdb ./redis-backup.rdb

# 清理未使用的数据卷
docker volume prune -f
```

### 扩缩容

```bash
# 扩展服务实例数
docker-compose up -d --scale [service-name]=3

# 查看扩展后的状态
docker-compose ps
```

---

## 环境配置

### 必需环境变量

```bash
# .env 文件模板

# ==================== 通用配置 ====================
NODE_ENV=development
PROJECT_NAME=major-guidance-app
LOG_LEVEL=debug

# ==================== 数据库配置 ====================
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_password
POSTGRES_DB=employment
DATABASE_URL=postgresql://postgres:your_secure_password@postgres:5432/employment

# ==================== Redis配置 ====================
REDIS_URL=redis://redis:6379
REDIS_PASSWORD=

# ==================== Elasticsearch配置 ====================
ELASTICSEARCH_URL=http://elasticsearch:9200
ELASTICSEARCH_USERNAME=
ELASTICSEARCH_PASSWORD=

# ==================== MinIO配置 ====================
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_ENDPOINT=localhost:9000
MINIO_BUCKET=uploads
MINIO_ACCESS_KEY=your_access_key
MINIO_SECRET_KEY=your_secret_key

# ==================== Kafka配置 ====================
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
KAFKA_USERNAME=
KAFKA_PASSWORD=

# ==================== API密钥 ====================
OPENAI_API_KEY=your_openai_api_key
ELEVENLABS_API_KEY=your_elevenlabs_api_key

# ==================== 邮件服务配置 ====================
SMTP_HOST=smtp.example.com
SMTP_PORT=587
SMTP_USER=your_email@example.com
SMTP_PASSWORD=your_email_password
EMAIL_FROM=noreply@major-app.com

# ==================== 前端配置 ====================
VITE_API_BASE_URL=http://localhost:80/api
VITE_WS_URL=ws://localhost:80/ws

# ==================== 爬虫配置 ====================
CRAWLER_USER_AGENT=Mozilla/5.0 (compatible; MajorApp/1.0)
CRAWLER_DELAY=2
CRAWLER_MAX_CONCURRENT=5
```

### 开发环境配置

```yaml
# docker-compose.dev.yml
version: '3.8'

services:
  # 开发环境特殊配置
  backend:
    volumes:
      - ./backend:/app
      - /app/node_modules
    environment:
      - DEBUG=1
      - HOT_RELOAD=1
    
  frontend:
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_HOT_RELOAD=1
```

### 生产环境配置

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # 生产环境配置
  backend:
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    environment:
      - NODE_ENV=production
      - LOG_LEVEL=info
    
  frontend:
    build:
      target: production
```

---

## 架构图

### 系统架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              客户端层                                        │
├─────────────┬─────────────┬─────────────┬─────────────┬─────────────────────┤
│   Web端      │   iOS端      │  Android端   │   小程序      │  第三方集成          │
│  (React)    │ (React Nat) │ (React Nat) │   (Vue)     │  (API)              │
└──────┬──────┴──────┬──────┴──────┬──────┴──────┬──────┴──────────┬──────────┘
       │             │             │             │                  │
       └─────────────┴──────┬──────┴─────────────┴──────────────────┘
                            │
                            ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                           API Gateway (Nginx)                                │
│                    路由分发 | 认证授权 | 限流熔断 | 日志                      │
└──────────────────────────────────┬──────────────────────────────────────────┘
                                   │
        ┌──────────────────────────┼──────────────────────────┐
        ↓                          ↓                          ↓
┌───────────────┐      ┌─────────────────────┐      ┌────────────────────────┐
│   用户服务      │      │      语音服务        │      │      对话服务           │
│  user-service  │      │   voice-service     │      │    chat-service        │
│  :8001         │      │     :8002           │      │      :8003             │
└───────┬───────┘      └──────────┬──────────┘      └───────────┬────────────┘
        │                         │                             │
        └─────────────────────────┼─────────────────────────────┘
                                  ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Kafka Message Queue                                │
│              用户事件 | 对话事件 | 推荐事件 | 通知事件 | 分析事件              │
└──────────────────────────────────┬──────────────────────────────────────────┘
                                   │
        ┌──────────────────────────┼──────────────────────────┐
        ↓                          ↓                          ↓
┌───────────────────┐  ┌─────────────────────────┐  ┌─────────────────────────┐
│    爬虫服务         │  │      专业服务            │  │      推荐服务            │
│  crawler-service   │  │    major-service        │  │ recommendation-service  │
│      :8004         │  │       :8005             │  │        :8006            │
└─────────┬─────────┘  └────────────┬────────────┘  └────────────┬────────────┘
          │                         │                             │
          └─────────────────────────┼─────────────────────────────┘
                                    ↓
┌─────────────────────────────────────────────────────────────────────────────┐
│                              数据存储层                                      │
├─────────────┬─────────────┬─────────────┬─────────────┬─────────────────────┤
│  PostgreSQL │    Redis    │ Elasticsearch│    MinIO    │    Chroma DB        │
│  (用户/专业)  │  (缓存/会话) │  (搜索/日志)  │  (文件存储)  │    (向量存储)        │
└─────────────┴─────────────┴─────────────┴─────────────┴─────────────────────┘
```

### 数据流架构图

```
学生语音输入
    │
    ├──→ ASR语音识别 ──→ 文本
    │
    ├──→ 意图识别 ──→ 意图分类
    │
    ├──→ 情感分析 ──→ 情绪判断
    │
    ├──→ 上下文理解 ──→ 状态追踪
    │
    ├──→ 知识检索（专业信息）──→ 相关专业
    │
    ├──→ 推荐引擎（专业推荐）──→ 专业列表
    │
    ├──→ 数据分析（趋势分析）──→ 趋势报告
    │
    ├──→ 爬虫引擎（新信息）──→ 最新数据
    │
    └──→ LLM生成 ──→ 回复文本
              │
              ├──→ 情感表达 ──→ TTS语音合成
              │
              ├──→ 结构化数据 ──→ 图表渲染
              │
              ├──→ 视频脚本 ──→ 视频生成
              │
              └──→ 邮件内容 ──→ 邮件发送
```

---

## 详细功能设计

### 语音交互功能

#### 功能描述

语音交互是本应用的核心交互方式，学生可以通过语音与应用进行自然对话，获得专业选择指导服务。

#### 技术方案

| 功能 | 技术选型 | 说明 |
|-----|---------|------|
| 语音识别 | OpenAI Whisper | 高准确率、多语言支持 |
| 语音合成 | ElevenLabs | 自然度高、支持情感控制 |
| 语音增强 | WebRTC AEC | 回声消除、噪声抑制 |
| 语音缓存 | Redis | 低延迟语音流传输 |

#### 接口设计

```yaml
# 语音对话接口
POST /api/v1/voice/conversation
Request:
  - audio: binary (音频数据)
  - format: string (音频格式: mp3/wav/opus)
  - language: string (语言: zh-CN)
Response:
  - audio_response: binary (语音回复)
  - text_response: string (文本回复)
  - suggestions: array (后续建议)
```

### 智能助手功能

#### 功能描述

智能助手是应用的核心服务，具备专业的学科选择知识，能够理解学生需求，提供个性化建议，并在对话中展现情感共鸣和人文关怀。

#### 技术方案

| 功能 | 技术选型 | 说明 |
|-----|---------|------|
| 对话模型 | GPT-4 + Fine-tuned | 专业化专业指导 |
| 提示工程 | LangChain | 结构化提示模板 |
| 知识检索 | Vector Store (Chroma) | 专业知识向量检索 |
| 上下文管理 | Redis + Session | 多轮对话状态维护 |

#### 心理引导策略

```python
# 对话策略示例
class CounselingStrategy:
    async def handle_choice_anxiety(self, user_message, context):
        # 1. 识别学生情绪
        emotion = await self.analyze_emotion(user_message)
        
        # 2. 根据情绪类型选择回应策略
        strategies = {
            'anxious': self.calming_response,      # 焦虑 → 安抚
            'confused': self.guidance_response,    # 困惑 → 引导
            'frustrated': self.encouragement_response, # 沮丧 → 鼓励
            'confident': self.affirmation_response,   # 自信 → 肯定
            'indecisive': self.exploration_response   # 犹豫 → 探索
        }
        
        # 3. 生成回应
        response = await strategies.get(emotion, self.neutral_response)(user_message, context)
        
        # 4. 添加共情表达
        response = self.add_empathy(response, emotion)
        
        return response
```

### 信息爬取功能

#### 功能描述

系统自动从主流教育网站和高考信息平台爬取专业信息，包括专业设置、课程内容、就业方向、录取分数线等，为学生提供最新的专业信息。

#### 技术方案

| 功能 | 技术选型 | 说明 |
|-----|---------|------|
| 爬虫框架 | Scrapy + Playwright | 异步爬取 + JS渲染 |
| 数据解析 | BeautifulSoup4 + lxml | HTML/XML解析 |
| 数据清洗 | Pandas + NumPy | 数据标准化 |
| 反爬处理 | Rotating Proxies + User-Agent | IP轮换 + 请求伪装 |

#### 爬取目标

| 网站 | 数据类型 | 更新频率 |
|-----|---------|---------|
| 阳光高考 | 专业信息、院校信息 | 每日 |
| 中国教育在线 | 专业介绍、分数线 | 每日 |
| 高考志愿填报 | 专业排名、就业数据 | 每日 |
| 各高校官网 | 招生简章、专业设置 | 每周 |

#### 自动爬取与数据更新机制

##### 功能概述

系统自动定期爬取专业行情数据，确保数据库中始终存储最新的专业信息，为学生提供准确、及时的专业选择参考。

##### 爬取策略

| 参数 | 值 | 说明 |
|-----|------|------|
| 爬取周期 | 每3天 | 系统自动执行 |
| 单次爬取上限 | 500条 | 避免单次任务过重 |
| 数据库最大存储 | 10,000条 | 自动清理旧数据 |
| 数据去重 | 是 | 根据URL+标题去重 |

##### 学科配额规则

系统采用**学科配额制**爬取策略，确保热门学科数据充足，冷门学科也有覆盖：

| 学科类别 | 配额 | 优先级 | 说明 |
|---------|------|--------|------|
| 工学 | 100条 | 10（最高） | 最热门，配额最多 |
| 理学 | 80条 | 9 | 热门专业 |
| 经济学 | 80条 | 9 | 热门专业 |
| 管理学 | 70条 | 8 | 中等热门 |
| 医学 | 60条 | 7 | 重要学科 |
| 法学 | 60条 | 7 | 重要学科 |
| 文学 | 50条 | 6 | 中等学科 |
| 教育学 | 50条 | 6 | 中等学科 |
| 艺术学 | 40条 | 5 | 艺术类 |
| 哲学 | 30条 | 4 | 冷门学科 |
| 历史学 | 30条 | 4 | 冷门学科 |
| 农学 | 30条 | 4 | 冷门学科 |
| 军事学 | 20条 | 3（最低） | 最冷门 |

**配额规则说明**：
1. **总量控制**：所有学科数据总和不超过10,000条
2. **学科上限**：每个学科最多爬取指定配额（如工学最多100条）
3. **优先级**：热门学科优先爬取，低优先级学科在配额用满后跳过
4. **动态调整**：可根据实际需求调整各学科配额

##### 配额管理器

```python
class CrawlerQuotaManager:
    """爬虫配额管理器"""
    
    SUBJECT_QUOTAS = {
        "工学": {"quota": 100, "priority": 10},
        "理学": {"quota": 80, "priority": 9},
        "经济学": {"quota": 80, "priority": 9},
        "管理学": {"quota": 70, "priority": 8},
        "医学": {"quota": 60, "priority": 7},
        "法学": {"quota": 60, "priority": 7},
        "文学": {"quota": 50, "priority": 6},
        "教育学": {"quota": 50, "priority": 6},
        "艺术学": {"quota": 40, "priority": 5},
        "哲学": {"quota": 30, "priority": 4},
        "历史学": {"quota": 30, "priority": 4},
        "农学": {"quota": 30, "priority": 4},
        "军事学": {"quota": 20, "priority": 3},
    }
    
    def can_crawl(self, category: str) -> bool:
        """检查是否可以爬取该学科"""
        pass
    
    def allocate_quota(self, category: str, count: int = 1) -> bool:
        """分配配额"""
        pass
    
    def get_distribution_plan(self, total_items: int) -> Dict[str, int]:
        """获取爬取分配计划（按学科优先级分配）"""
        pass
```

##### 数据保存流程

```
爬取数据
    ↓
检查学科配额（每个学科上限）
    ↓
去重检查（URL+标题）
    ↓
分配配额并保存
    ↓
检查总数是否 > 10000
    ↓ 是 → 删除最旧数据 → 保持10000条
    ↓ 否 → 完成
```

##### API接口

```yaml
# 查看配额状态
GET /api/v1/crawler/quota
Response:
  total_max: 10000       # 最大总量
  total_used: 150        # 已使用
  total_remaining: 9850  # 剩余配额
  subjects:
    工学: {max_quota: 100, current: 50, remaining: 50, priority: 10}
    ...

# 查看配额统计
GET /api/v1/crawler/statistics
Response:
  utilization_rate: 1.5  # 利用率百分比
  subjects:
    工学: {quota: 100, used: 50, rate: 50.0, priority: 10}
    ...
```

##### 自动执行机制

```python
# Celery定时任务配置
from celery import Celery
from celery.schedules import timedelta

app = Celery('crawler')

# 每3天执行一次爬虫任务
app.conf.beat_schedule = {
    'crawl-major-data-every-3-days': {
        'task': 'crawler.crawl_major_data',
        'schedule': timedelta(days=3),
        'options': {'queue': 'crawler'}
    }
}
```

##### 数据库数据管理策略

```python
class MajorDataManager:
    """专业数据管理器 - 保证数据库最多存储10000条最新数据"""
    
    MAX_RECORDS = 10000
    
    def save_crawled_data(self, new_data: List[Dict]):
        """
        保存爬取的数据，并确保数据库不超过最大记录数
        策略：
        1. 去重（根据URL+标题）
        2. 插入新数据
        3. 如果超过10000条，删除最旧的记录
        """
        # 1. 获取现有数据的URL集合（用于去重）
        existing_urls = self.get_existing_urls()
        
        # 2. 过滤掉重复数据
        unique_new_data = [
            item for item in new_data 
            if item['url'] not in existing_urls
        ]
        
        # 3. 批量插入新数据
        if unique_new_data:
            self.batch_insert(unique_new_data)
        
        # 4. 检查并清理旧数据
        self.ensure_max_records()
    
    def ensure_max_records(self):
        """确保数据库中只有最新的10000条数据"""
        current_count = self.get_record_count()
        
        if current_count > self.MAX_RECORDS:
            # 计算需要删除的数量
            excess = current_count - self.MAX_RECORDS
            
            # 删除最旧的excess条记录
            self.delete_oldest_records(excess)
            
            print(f"已清理 {excess} 条旧数据，当前数据库共有 {self.MAX_RECORDS} 条最新记录")
    
    def get_record_count(self) -> int:
        """获取当前数据条数"""
        pass
    
    def get_existing_urls(self) -> set:
        """获取所有现有数据的URL（用于去重）"""
        pass
    
    def batch_insert(self, data: List[Dict]):
        """批量插入数据"""
        pass
    
    def delete_oldest_records(self, count: int):
        """删除最旧的count条记录"""
        pass
```

##### 数据表设计

```sql
-- 专业行情数据表
CREATE TABLE major_market_data (
    id SERIAL PRIMARY KEY,
    title VARCHAR(500) NOT NULL,          -- 数据标题
    major_name VARCHAR(200),               -- 专业名称
    category VARCHAR(100),                 -- 学科门类
    source_url VARCHAR(1000) UNIQUE,       -- 来源URL（用于去重）
    source_website VARCHAR(100),           -- 来源网站
    employment_rate DECIMAL(5,2),          -- 就业率
    avg_salary VARCHAR(100),               -- 平均薪资
    admission_score DECIMAL(5,2),          -- 录取分数线
    heat_index DECIMAL(5,2),               -- 热度指数
    trend_data JSONB,                      -- 趋势数据（JSON）
    description TEXT,                      -- 详细描述
    courses JSONB,                         -- 核心课程
    career_prospects TEXT,                 -- 就业前景
    crawled_at TIMESTAMP DEFAULT NOW(),    -- 爬取时间
    updated_at TIMESTAMP DEFAULT NOW(),    -- 更新时间
    created_at TIMESTAMP DEFAULT NOW()     -- 创建时间
);

-- 创建索引
CREATE INDEX idx_major_market_crawled_at ON major_market_data(crawled_at DESC);
CREATE INDEX idx_major_market_major_name ON major_market_data(major_name);
CREATE INDEX idx_major_market_category ON major_market_data(category);

-- 创建触发器自动更新updated_at
CREATE TRIGGER update_major_market_data_updated_at
    BEFORE UPDATE ON major_market_data
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

##### 爬取任务状态监控

| 状态 | 说明 |
|-----|------|
| pending | 等待执行 |
| running | 执行中 |
| completed | 成功完成 |
| failed | 执行失败 |
| partially_completed | 部分完成（部分数据失败） |

##### API接口

```yaml
# 获取最新专业行情数据
GET /api/v1/major/market-data
Query Parameters:
  - page: int (default: 1)
  - page_size: int (default: 20, max: 100)
  - category: string (可选，学科门类筛选)
  - sort_by: string (可选，排序字段: crawled_at, heat_index, employment_rate)
  - order: string (可选，排序方向: asc, desc)

Response:
  data:
    - id: int
      major_name: string
      category: string
      employment_rate: float
      avg_salary: string
      heat_index: float
      crawled_at: datetime
  pagination:
    page: int
    page_size: int
    total: int
    total_pages: int

# 触发手动爬取（管理员接口）
POST /api/v1/admin/crawler/trigger
Request:
  - force: boolean (可选，强制立即执行，忽略调度周期)

Response:
  task_id: string
  status: string
  message: string

# 获取爬取任务状态
GET /api/v1/admin/crawler/status/{task_id}
Response:
  task_id: string
  status: string
  started_at: datetime
  completed_at: datetime
  records_crawled: int
  records_saved: int
  error_message: string (如果有)
```

##### 数据保留策略总结

| 数据类型 | 保留策略 |
|---------|---------|
| 专业行情数据 | 最多10000条，保留最新 |
| 临时爬取日志 | 保留7天 |
| 爬取失败记录 | 保留30天 |
| 统计分析数据 | 保留90天 |

---

### 数据流架构与更新机制

#### 核心数据流规则

本项目遵循严格的数据流架构，确保数据来源清晰、更新机制可靠：

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                              数据流架构图                                         │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐  │
│   │  爬虫Agent   │────▶│  PostgreSQL  │◀────│   后端API    │────▶│   前端展示   │  │
│   │  (定时触发)  │     │   数据库      │     │   (读取数据)  │     │   (Web/APP)  │  │
│   └─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘  │
│         │                   │                   │                   │           │
│         │                   │     ┌─────────────┘                   │           │
│         │                   │     │                               │           │
│         │                   ▼     ▼                               │           │
│         │           ┌─────────────────┐                           │           │
│         │           │     Redis       │◀──────────────────────────┘           │
│         │           │   (缓存层)       │                                       │
│         │           └─────────────────┘                                       │
│         │                                                                     │
│         ▼                                                                     │
│   数据写入数据库                                                               │
│   (去重、清理旧数据)                                                           │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

#### 数据源原则

| 原则 | 说明 | 违规示例 |
|------|------|---------|
| **数据来源唯一** | 所有展示数据必须来自数据库或Redis缓存 | ❌ 后端直接爬取数据返回给前端 |
| **爬取与展示分离** | 爬虫负责写库，后端负责读库 | ❌ 前端直接调用爬虫接口 |
| **定时更新** | 数据由调度器周期性更新 | ❌ 用户请求时实时爬取 |
| **强制爬取** | 全量爬取指令强制刷新所有数据 | - |

#### 数据流规则

##### 规则1：爬虫Agent负责数据采集

| 规则 | 说明 |
|------|------|
| 触发方式 | 由调度服务定时触发 |
| 采集内容 | 专业信息、大学信息、行业趋势、视频等 |
| 输出目标 | 写入PostgreSQL数据库 |
| 禁止行为 | **后端服务禁止直接从网络爬取数据返回给前端** |

##### 规则2：后端API只读数据

| 规则 | 说明 |
|------|------|
| 数据来源 | PostgreSQL数据库 或 Redis缓存 |
| 缓存策略 | 热门数据缓存到Redis，减少数据库压力 |
| 禁止行为 | **后端API不能直接调用爬虫功能或从网络实时获取数据** |

##### 规则3：前端通过API获取数据

| 规则 | 说明 |
|------|------|
| 数据来源 | 后端API |
| 后端数据来源 | 数据库/Redis |
| 禁止行为 | **前端禁止直接调用爬虫接口或第三方API** |

#### 定时调度机制

##### 调度服务设计

```yaml
调度服务（Scheduler Service）
  - 职责：管理所有数据的定时更新任务
  - 技术栈：Python + Celery + Redis Queue
  - 触发方式：Celery Beat 定时任务
```

##### 数据更新周期

| 数据类型 | 更新周期 | 触发时间 | 爬虫Agent |
|---------|---------|---------|----------|
| 热门专业行情 | 每3天 | 凌晨2:00 | data-crawler |
| 大学录取分数 | 每周 | 每周日凌晨3:00 | data-crawler |
| 行业趋势 | 每日 | 每天上午6:00 | data-crawler |
| 视频内容 | 每周 | 每周六上午4:00 | video-processor |
| 学科分类 | 每月 | 每月1日凌晨1:00 | data-crawler |
| 爬取配额重置 | 每3天 | 与专业行情同步 | data-crawler |

##### 调度配置文件

```python
# scheduler_config.py
SCHEDULER_CONFIG = {
    "major_market_data": {
        "schedule": "interval",  # 间隔调度
        "hours": 72,  # 每3天
        "trigger_time": "02:00",  # 凌晨2点
        "crawler_agent": "data-crawler",
        "priority": 1,  # 优先级
        "full_crawl": False  # 默认增量爬取
    },
    "university_admission_scores": {
        "schedule": "cron",  # Cron调度
        "day_of_week": 0,  # 每周日
        "hour": 3,  # 凌晨3点
        "crawler_agent": "data-crawler",
        "priority": 2,
        "full_crawl": False
    },
    "industry_trends": {
        "schedule": "cron",
        "hour": 6,  # 每天上午6点
        "crawler_agent": "data-crawler",
        "priority": 3,
        "full_crawl": False
    },
    "video_content": {
        "schedule": "cron",
        "day_of_week": 6,  # 每周六
        "hour": 4,  # 凌晨4点
        "crawler_agent": "video-processor",
        "priority": 4,
        "full_crawl": False
    },
    "major_categories": {
        "schedule": "cron",
        "day_of_month": 1,  # 每月1日
        "hour": 1,  # 凌晨1点
        "crawler_agent": "data-crawler",
        "priority": 5,
        "full_crawl": False
    },
    "quota_reset": {
        "schedule": "interval",
        "hours": 72,  # 与专业行情同步
        "trigger_time": "02:00",
        "crawler_agent": "data-crawler",
        "priority": 6,
        "full_crawl": False
    }
}
```

#### 全量爬取机制

##### 功能描述

当用户或管理员执行**全量爬取指令**时，爬虫Agent必须强制重新爬取所有数据，忽略增量爬取逻辑，确保数据的完整性和一致性。

##### 全量爬取触发方式

| 触发方式 | 触发条件 | 使用场景 |
|---------|---------|---------|
| API指令 | POST /api/v1/admin/crawler/full-crawl | 管理员手动触发 |
| 命令行 | python manage.py crawl --full | 开发/运维人员 |
| 调度器 | 标记为全量爬取的任务 | 紧急数据更新 |

##### 全量爬取规则

| 规则 | 说明 |
|------|------|
| **忽略增量逻辑** | 不检查数据是否已存在，强制重新爬取 |
| **覆盖旧数据** | 根据source_url覆盖已有记录 |
| **重置配额** | 爬取前重置所有学科配额使用计数 |
| **更新所有数据** | 包括热门学科和冷门学科 |
| **记录操作日志** | 标记为全量爬取，便于追溯 |

##### 全量爬取API接口

```yaml
# 触发全量爬取
POST /api/v1/admin/crawler/full-crawl
Request Body:
  - task_type: string (可选, 默认: all)
    - all: 全量爬取所有数据
    - major: 只爬取专业数据
    - university: 只爬取大学数据
    - video: 只爬取视频数据
    - trend: 只爬取行业趋势

Response:
  task_id: string  # 任务ID
  status: string   # started
  message: string  # "全量爬取任务已启动"
  crawl_mode: string  # "full"
  estimated_time: int  # 预估完成时间(分钟)

# 查询全量爬取状态
GET /api/v1/admin/crawler/status/{task_id}
Response:
  task_id: string
  status: string  # pending/running/completed/failed
  crawl_mode: string  # "full"
  progress: object
    - current: int  # 当前处理数量
    - total: int    # 总数量
    - percentage: float  # 完成百分比
  records_crawled: int  # 已爬取数量
  records_saved: int    # 已保存数量
  started_at: datetime
  completed_at: datetime  # 完成时间
  error_message: string   # 错误信息
```

##### 全量爬取执行流程

```
1. 接收全量爬取指令
    ↓
2. 重置所有学科配额使用计数
    ↓
3. 标记任务为full模式
    ↓
4. 遍历所有数据源进行爬取
    ↓
5. 强制覆盖已有数据（根据source_url）
    ↓
6. 记录全量爬取日志
    ↓
7. 返回完成状态
```

##### 全量爬取 vs 增量爬取对比

| 对比项 | 全量爬取 | 增量爬取 |
|-------|---------|---------|
| 触发条件 | 手动指令或紧急更新 | 定时调度 |
| 数据范围 | 所有学科 | 热门学科优先 |
| 配额使用 | 重置后重新使用 | 受剩余配额限制 |
| 数据处理 | 强制覆盖已有数据 | 根据URL去重 |
| 执行时间 | 较长（可能数小时） | 较短（通常30分钟内） |
| 使用场景 | 数据异常修复、重大更新 | 常规数据更新 |

#### Redis缓存层设计

##### 缓存策略

| 缓存类型 | 缓存键 | TTL | 说明 |
|---------|-------|-----|------|
| 学科分类 | `categories:all` | 24小时 | 变化频率低 |
| 专业列表 | `majors:list:{page}:{category}` | 12小时 | 分页数据 |
| 专业详情 | `majors:{id}` | 6小时 | 单个专业 |
| 大学列表 | `universities:list:{page}:{province}` | 12小时 | 分页数据 |
| 大学详情 | `universities:{id}` | 6小时 | 单个大学 |
| 录取分数 | `admission:{university_id}:{year}` | 24小时 | 录取数据 |
| 行业趋势 | `trends:{industry}` | 12小时 | 趋势数据 |
| 视频列表 | `videos:{major}:{page}` | 24小时 | 视频数据 |
| 爬取配额 | `quota:status` | 1小时 | 配额状态 |

##### 缓存API接口

```yaml
# 获取缓存数据（带缓存穿透保护）
GET /api/v1/data/categories
Response Headers:
  X-Cache: HIT/MISS  # 缓存命中状态
  X-Cache-TTL: 86400  # 剩余TTL(秒)

# 手动清除缓存（管理员接口）
DELETE /api/v1/admin/cache/{cache_key}
Response:
  status: string  # success
  message: string  # "缓存已清除"

# 批量清除缓存
DELETE /api/v1/admin/cache/bulk
Request Body:
  - pattern: string  # 缓存键模式，如 "majors:*"

# 获取缓存统计
GET /api/v1/admin/cache/stats
Response:
  hit_rate: float   # 缓存命中率
  memory_usage: int # 内存使用量(字节)
  keys_count: int   # 缓存键数量
```

##### 缓存更新策略

| 策略 | 说明 | 适用场景 |
|-----|------|---------|
| **被动过期** | TTL到期自动失效 | 大多数数据 |
| **主动更新** | 爬取完成后主动刷新缓存 | 热门数据 |
| **分层缓存** | 多级缓存(L1:内存, L2:Redis) | 高频访问数据 |
| **缓存预热** | 定时任务提前加载热点数据 | 重要数据 |

#### 数据一致性保障

##### 写入流程（爬虫Agent）

```
爬虫Agent爬取数据
    ↓
数据清洗和验证
    ↓
PostgreSQL数据库写入（去重、清理旧数据）
    ↓
Redis缓存失效（相关数据的缓存键）
    ↓
记录爬取历史
    ↓
更新爬取配额
```

##### 读取流程（后端API）

```
前端请求API
    ↓
检查Redis缓存
    ↓
┌─────────────────┐
│  缓存命中       │──▶返回缓存数据
│  缓存未命中     │
└─────────────────┘
    ↓
从PostgreSQL读取数据
    ↓
写入Redis缓存（设置TTL）
    ↓
返回数据给前端
```

##### 数据一致性规则

| 场景 | 处理方式 |
|-----|---------|
| 爬取完成后 | 自动清除相关Redis缓存 |
| 缓存过期 | 下次请求时从数据库加载 |
| 数据库更新 | 同时更新Redis缓存或标记失效 |
| 缓存穿透 | 添加空值缓存或布隆过滤器 |

---

### 推荐功能

#### 功能描述

基于学生画像和行为数据，为学生推荐适合的专业方向。推荐过程是引导式的，帮助学生逐步明确专业方向。

#### 技术方案

| 功能 | 技术选型 | 说明 |
|-----|---------|------|
| 学生画像 | Spark MLlib | 多维度特征工程 |
| 推荐算法 | 协同过滤 + 内容推荐 | 混合推荐策略 |
| 匹配引擎 | Elasticsearch | 快速相似度计算 |
| 解释生成 | LLM | 推荐理由生成 |

### 专业详情与大学推荐功能

#### 功能描述

在专业推荐列表中，点击"查看详情"按钮后，弹出专业详情弹窗。弹窗展示专业的详细信息，并基于学生的目标省份和预估分数，推荐个性化的大学列表。

#### 功能特性

| 功能模块 | 说明 |
|---------|------|
| 专业详情展示 | 展示专业名称、学科门类、热度指数、就业率、薪资、核心课程等 |
| 数据概览卡片 | 就业率、平均薪资、热度指数三个关键指标可视化 |
| 个性化大学推荐 | 根据用户设置的目标省份和预估分数，推荐匹配的大学 |
| 全国优质大学 | 展示该专业全国范围内最好的大学 |
| 核心课程展示 | 展示该专业的核心课程标签 |
| 专业介绍与就业前景 | 文本形式展示专业详细信息 |

#### 用户交互流程

```
用户点击"查看详情"
    ↓
显示专业详情弹窗
    ↓
判断是否已设置用户目标（省份+分数）
    ↓
┌────────────────────────────────────────┐
│  未设置目标：显示设置入口                │
│  "设置您的省份和分数，获取个性化推荐"     │
└────────────────────────────────────────┘
    ↓
用户设置：江苏省 + 620分
    ↓
后端返回个性化推荐数据
    ↓
前端分类展示：
  - 🏆 分数匹配大学（620±30分范围内）
  - 📍 同省优质大学（江苏省内）
  - 🏆 全国推荐大学（清华北大浙大等）
```

#### 数据模型设计

```typescript
// 大学信息
interface University {
  id: number;
  name: string;           // 大学名称
  level: string;          // 985/211/双一流（多标签用逗号分隔）
  location: string;       // 所在城市
  province: string;       // 所在省份
  employmentRate: string; // 就业率
  majorStrength: string;  // 专业优势描述
}

// 录取分数线
interface AdmissionLine {
  university: string;
  province: string;
  minScore: number;       // 最低录取分
  maxScore: number;       // 最高录取分
  year: number;           // 年份
  admissionType: string;  // 录取类型（本科一批/提前批等）
}

// 专业推荐的大学
interface RecommendedUniversity {
  university: University;
  admissionLine?: AdmissionLine;  // 录取分数线（分数匹配时需要）
  matchType: 'score' | 'province' | 'national' | 'strength';
  matchReason: string;           // 匹配原因描述
}

// 专业详情数据
interface MajorDetail {
  id: number;
  name: string;              // 专业名称
  category: string;          // 学科门类
  heatIndex: number;         // 热度指数
  employmentRate: number;    // 就业率
  avgSalary: string;         // 平均薪资
  courses: string[];         // 核心课程
  description: string;       // 专业介绍
  careerProspects: string;   // 就业前景
  recommendedUniversities: RecommendedUniversity[];  // 推荐大学列表
}
```

#### UI设计规范

##### 专业详情弹窗布局

```
┌─────────────────────────────────────────────────────────┐
│  💼 计算机科学与技术                          [✖ 关闭] │
│     [工学] [匹配98.5%]                                    │
├─────────────────────────────────────────────────────────┤
│  📊 数据概览                                                  │
│  ┌─────────────┬─────────────┬─────────────┐              │
│  │    就业率    │   平均薪资   │   热度指数   │              │
│  │    95.5%    │ 18K-25K/月   │    98.5     │              │
│  └─────────────┴─────────────┴─────────────┘              │
├─────────────────────────────────────────────────────────┤
│  🏆 个性化推荐大学                                           │
│                                                             │
│  📍 您的目标：江苏省 · 预估分数 620分        [⚙️ 修改目标] │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 🏆 分数匹配大学                                       │   │
│  │ 南京大学      985/211 | 录取分 650+  就业率 98%      │   │
│  │ 东南大学      985/211 | 录取分 640+  就业率 96%      │   │
│  │ 南京航空航天  211     | 录取分 620+  就业率 95%      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 📍 同省优质大学                                       │   │
│  │ 南京大学      985/211 | 就业率 98% | 南京           │   │
│  │ 苏州大学      211     | 就业率 95% | 苏州           │   │
│  │ 河海大学      211     | 就业率 94% | 南京           │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 🏆 全国推荐大学                                       │   │
│  │ 清华大学      985/211 | 就业率 99% | 北京           │   │
│  │ 北京大学      985/211 | 就业率 98% | 北京           │   │
│  │ 浙江大学      985/211 | 就业率 97% | 杭州           │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────┤
│  📚 核心课程                                                 │
│  [数据结构] [算法分析] [操作系统] [计算机网络] [数据库原理]│
├─────────────────────────────────────────────────────────┤
│  💡 专业介绍                                                 │
│  计算机科学与技术专业培养具有良好的科学素养，系统掌握...   │
├─────────────────────────────────────────────────────────┤
│  🎯 就业前景                                                 │
│  毕业生可在IT企业、科研单位、软件开发公司等从事软件...     │
└─────────────────────────────────────────────────────────┘
```

##### 标签设计规范

| 标签类型 | 样式 | 示例 |
|---------|------|------|
| 985 | 蓝色边框 + 蓝色文字 | `[985]` |
| 211 | 绿色边框 + 绿色文字 | `[211]` |
| 双一流 | 紫色边框 + 紫色文字 | `[双一流]` |
| 就业率 | 绿色背景 | `就业率 98%` |
| 录取分 | 蓝色背景 | `录取分 650+` |

##### 专业注意事项设计

专业详情弹窗中需展示该专业的**注意事项、风险提示、发展建议**等细节信息，帮助学生全面了解专业，避免盲目选择。

###### 设计原则

| 原则 | 说明 |
|------|------|
| 真实性 | 如实展示专业的挑战和风险，不夸大优点 |
| 平衡性 | 优点和注意事项都要展示 |
| 针对性 | 不同专业的注意事项不同 |
| 建设性 | 给出建议，帮助学生应对挑战 |

###### 专业注意事项分类

| 类别 | 说明 | 示例 |
|------|------|------|
| 💰 薪资与工作强度 | 初始薪资、工作节奏、加班情况 | "起薪高但工作强度大，996是常态" |
| 📚 学历要求 | 对学历的硬性要求 | "需读到博士，硕士就业压力大" |
| 🔄 职业稳定性 | 行业变化、年龄门槛、裁员风险 | "35岁后面临职业转型压力" |
| 📈 发展空间 | 晋升路径、技能更新、行业前景 | "技术迭代快，需持续学习" |
| 🎯 就业门槛 | 入门难度、证书要求、实习经历 | "需持证上岗，通过率低" |

###### UI设计规范

```
┌─────────────────────────────────────────────────────────┐
│  ⚠️ 注意事项                                              │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  💰 薪资与工作强度                                       │
│  ▶ 起薪较高，但工作强度大，加班是常态                    │
│  ▶ 薪资与个人能力挂钩，差距较大                          │
│                                                         │
│  🔄 职业稳定性                                           │
│  ▶ 35岁后可能面临职业转型或淘汰风险                      │
│  ▶ 行业变化快，需持续学习新技术                          │
│                                                         │
│  📈 发展建议                                             │
│  ▶ 建议在校期间多参与项目实践，积累经验                   │
│  ▶ 提前规划职业发展方向，不局限于技术路线                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

###### 专业注意事项示例

| 专业 | 注意事项类别 | 具体内容 |
|------|-------------|---------|
| 计算机科学与技术 | 职业稳定性 | 互联网行业裁员风险高，35岁后面临职业转型压力，技术迭代快需持续学习 |
| 计算机科学与技术 | 工作强度 | 加班文化普遍（996），项目上线期压力巨大，需适应高强度工作节奏 |
| 计算机科学与技术 | 发展空间 | 入门门槛低但精通难，建议深耕细分领域或转向管理/架构方向 |
| 临床医学 | 学历要求 | 需读到博士（三甲医院门槛），硕士就业压力大，本科基本无法进入好医院 |
| 临床医学 | 薪资与工作强度 | 规培期工资低（3-5年），工作强度大（夜班、值班），但后期稳定且收入持续上涨 |
| 临床医学 | 职业稳定性 | 一旦进入正规医院，工作非常稳定，越老越吃香，铁饭碗属性强 |
| 法学 | 学历要求 | 需通过法考（通过率约15%），红圈所对学历要求极高 |
| 法学 | 职业稳定性 | 案源是关键，独立执业前收入不稳定，律师行业二八定律明显 |
| 金融学 | 学历要求 | 头部机构只要清北复交，硕士是起步学历，竞争极其激烈 |
| 金融学 | 薪资与工作强度 | 起薪高但压力大，考核指标重，人脉资源很重要 |
| 金融学 | 职业稳定性 | 行业周期性明显，牛市高薪熊市裁员，需有心理准备 |
| 师范类 | 学历要求 | 一线城市要求硕士学历，编制考试竞争激烈（100:1常见） |
| 师范类 | 职业稳定性 | 有编制则非常稳定，无编制则流动性大，寒暑假是最大福利 |
| 师范类 | 薪资与工作强度 | 基本工资不高但福利齐全，工作时间固定（除班主任外） |
| 艺术设计 | 职业稳定性 | 自由职业为主，收入不稳定，需持续经营个人品牌 |
| 艺术设计 | 学历要求 | 作品集比学历更重要，但头部机构仍看学历背景 |
| 会计学 | 学历要求 | CPA是核心证书，难度大（通过率约15%），考出后薪资翻倍 |
| 会计学 | 职业稳定性 | 企业财务是刚需，永远不会被AI完全替代，越老越值钱 |

###### 数据模型设计

```typescript
// 专业注意事项
interface MajorNote {
  category: 'salary' | 'education' | 'stability' | 'development' | 'threshold';
  categoryLabel: string;  // 分类标签，如 "薪资与工作强度"
  icon: string;           // 图标，如 "💰"
  points: string[];       // 要点列表
  suggestions?: string[]; // 发展建议（可选）
}

// 专业详情数据扩展
interface MajorDetail {
  id: number;
  name: string;
  // ... 其他字段
  notes: MajorNote[];     // 专业注意事项列表
}
```

###### 注意事项内容来源

| 来源 | 说明 |
|------|------|
| 行业报告 | 麦可思就业质量报告、薪酬调查报告 |
| 从业者访谈 | 真实从业者的工作体验分享 |
| 媒体报道 | 行业新闻、裁员报道、薪资分析 |
| 专家建议 | 教育专家、职业规划师的专业意见 |

##### 筛选表单设计

```
┌─────────────────────────────────────────┐
│  🎯 设置您的目标                         │
│                                         │
│  目标省份：  [江苏 ▼]                    │
│  预估分数：  [____] 分                   │
│                                         │
│           [取消]  [确认应用]             │
└─────────────────────────────────────────┘
```

#### 用户目标存储策略

| 存储方式 | 适用场景 | 实现方式 |
|---------|---------|---------|
| 临时输入 | 游客用户，一次性使用 | React State |
| localStorage | 跨页面保持 | 浏览器本地存储 |
| 用户账户 | 已登录用户 | 后端数据库存储 |

#### 推荐算法规则

##### 分数匹配大学（±30分范围）

```
用户预估分数 = 620分
    ↓
匹配区间 = 590分 ~ 650分
    ↓
筛选条件：
  1. 在该省份有招生
  2. 录取分数线在匹配区间内
  3. 按就业率降序排序
    ↓
结果示例：
  南京大学（650+）
  东南大学（640+）
  南京航空航天（620+）
```

##### 同省优质大学

```
用户目标省份 = 江苏省
    ↓
筛选条件：
  1. 大学所在省份 = 江苏省
  2. 该专业在大学中为优势专业
  3. 按就业率降序排序
    ↓
结果示例：
  南京大学（南京，就业率98%）
  苏州大学（苏州，就业率95%）
  河海大学（南京，就业率94%）
```

##### 全国推荐大学

```
筛选条件：
  1. 该专业全国排名前10
  2. 就业率 >= 95%
  3. 按热度指数降序排序
    ↓
结果示例：
  清华大学（就业率99%）
  北京大学（就业率98%）
  浙江大学（就业率97%）
```

#### 推荐算法规则

##### 用户目标设置规则

| 场景 | 触发条件 | 行为 |
|-----|---------|------|
| **首次进入推荐大学** | 未设置过用户目标 | 自动弹出目标设置弹窗 |
| **再次进入推荐大学** | 已设置过用户目标 | 不弹窗，直接使用已保存的目标 |
| **手动修改目标** | 用户点击"修改目标"按钮 | 弹出目标设置弹窗，允许修改 |

##### 目标设置弹窗规则

| 字段 | 是否必填 | 说明 |
|-----|---------|------|
| 目标省份 | 可选 | 可选择任意省份或选择"不设置省份" |
| 预估分数 | 可选 | 可输入预估分数或选择"不设置分数" |

##### 大学推荐规则（三层逻辑）

根据用户是否设置省份和分数，大学推荐分为以下三种场景：

| 场景 | 条件 | 推荐逻辑 | 推荐结果 |
|-----|------|---------|---------|
| **场景A** | 省份 + 分数 + 专业 | 1. 同省分数匹配大学（620±30分）<br>2. 全国分数和专业匹配大学 | 同省分数匹配 + 全国匹配 |
| **场景B** | 只有省份 + 专业 | 1. 同省该专业优质大学<br>2. 全国该专业优质大学（补充至5所） | 同省优质 + 全国优质 |
| **场景C** | 什么都没填 + 专业 | 按专业匹配度和就业率排序 | 全国优质大学 |

##### 各场景详细规则

**场景A：省份+分数+专业（完整目标）**
```
用户：江苏省 + 620分 + 计算机科学与技术

推荐逻辑：
1. 同省分数匹配大学：江苏省内录取分在590~650分的计算机相关专业
   - 筛选条件：省份=江苏，分数=620±30，专业匹配度>0
   - 排序：分数匹配度降序 > 专业匹配度降序
   - 数量：3-5所

2. 全国分数和专业匹配大学：全国范围内符合分数和专业的大学
   - 筛选条件：分数=620±30，专业匹配度>0
   - 排序：分数匹配度降序 > 就业率降序
   - 数量：2-3所（如同省已满5所则不显示）

展示分组：
┌─────────────────────────────────────────────────────┐
│ 🏆 分数匹配大学（江苏）                               │
│   南京大学      985/211 | 录取分 650+  就业率 98%   │
│   东南大学      985/211 | 录取分 640+  就业率 96%   │
│   南京航空航天  211     | 录取分 620+  就业率 95%   │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│ 🏆 全国匹配大学                                       │
│   清华大学      985/211 | 就业率 99% | 北京         │
│   北京大学      985/211 | 就业率 98% | 北京         │
│   浙江大学      985/211 | 就业率 97% | 杭州         │
└─────────────────────────────────────────────────────┘
```

**场景B：只有省份+专业（无分数）**
```
用户：江苏省 + 计算机科学与技术（未填分数）

推荐逻辑：
1. 同省优质大学：江苏省内计算机科学与技术专业匹配度>0的大学
   - 筛选条件：省份=江苏，专业匹配度>0
   - 排序：专业匹配度降序 > 就业率降序
   - 数量：3-5所

2. 全国优质大学：全国范围内计算机科学与技术专业匹配度高的大学
   - 筛选条件：专业匹配度>0（排除已显示的同省大学）
   - 排序：专业匹配度降序 > 就业率降序
   - 数量：2-3所（如同省已满5所则不显示）

展示分组：
┌─────────────────────────────────────────────────────┐
│ 📍 江苏优质大学                                       │
│   南京大学      985/211 | 就业率 98% | 南京         │
│   东南大学      985/211 | 就业率 96% | 南京         │
│   苏州大学      211     | 就业率 95% | 苏州         │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│ 🏆 全国推荐大学                                       │
│   清华大学      985/211 | 就业率 99% | 北京         │
│   北京大学      985/211 | 就业率 98% | 北京         │
│   浙江大学      985/211 | 就业率 97% | 杭州         │
└─────────────────────────────────────────────────────┘
```

**场景C：什么都没填+专业（默认值）**
```
用户：计算机科学与技术（无省份、无分数）

推荐逻辑：
1. 全国优质大学：计算机科学与技术专业匹配度最高的大学
   - 筛选条件：专业匹配度>0
   - 排序：专业匹配度降序 > 就业率降序 > 知名度
   - 数量：5所

展示分组：
┌─────────────────────────────────────────────────────┐
│ 🏆 全国推荐大学                                       │
│   清华大学      985/211 | 就业率 99% | 北京         │
│   北京大学      985/211 | 就业率 98% | 北京         │
│   浙江大学      985/211 | 就业率 97% | 杭州         │
│   上海交通大学  985/211 | 就业率 96% | 上海         │
│   复旦大学      985/211 | 就业率 95% | 上海         │
└─────────────────────────────────────────────────────┘
```

##### 分数匹配规则

| 参数 | 值 | 说明 |
|-----|------|------|
| 分数范围 | 用户分数 ± 30分 | 例如用户620分，匹配范围590~650分 |
| 排序方式 | 降序 | 分数越高越靠前 |
| 匹配度计算 | 基于大学王牌专业与目标专业的重合度 | 重合专业数>=2为100分 |

##### 专业匹配度计算

```
专业匹配度 = 基于大学王牌专业与目标专业的重合度

匹配度评分：
- 重合专业数 >= 2：100分
- 重合专业数 = 1：60分
- 同属一个学科门类：30分
- 无匹配：0分
```

##### 兜底规则

当所有场景都无法匹配到足够大学时，返回就业率最高的5所大学作为兜底。
用户：江苏省 + 620分 + 计算机科学与技术

推荐逻辑：
1. 筛选条件：省份=江苏，分数=620±30，专业匹配度>0
2. 排序：专业匹配度降序 > 分数匹配度降序
3. 数量：最多5所
4. 如不足5所，补充全国范围内符合条件的大学
```

**场景2：只有省份**
```
用户：江苏省 + 计算机科学与技术

推荐逻辑：
1. 第一优先级：江苏省内计算机科学与技术专业匹配度>0的大学
2. 第二优先级：全国范围内计算机科学与技术专业匹配度>0的大学（排除已显示的）
3. 排序：专业匹配度降序 > 就业率降序
4. 总数量：5所
```

**场景3：只有分数**
```
用户：620分

推荐逻辑：
1. 筛选条件：录取分数在590~650分范围内
2. 排序：录取分数降序（分数越高越靠前）
3. 数量：5所
```

**场景4：都没有设置**
```
用户：计算机科学与技术（无省份、无分数）

推荐逻辑：
1. 筛选条件：计算机科学与技术专业匹配度>0
2. 排序：专业匹配度降序 > 就业率降序（知名度从高到低）
3. 数量：5所
```

##### 分数匹配规则

| 参数 | 值 | 说明 |
|-----|------|------|
| 分数范围 | 用户分数 ± 30分 | 例如用户620分，匹配范围590~650分 |
| 排序方式 | 降序 | 分数越高越靠前 |

##### 专业匹配度计算

```
专业匹配度 = 基于大学王牌专业与目标专业的重合度

匹配度评分：
- 重合专业数 >= 2：100分
- 重合专业数 = 1：60分
- 同属一个学科门类：30分
- 无匹配：0分
```

##### 兜底规则

当所有场景都无法匹配到足够大学时，返回就业率最高的5所大学作为兜底。

##### 大学数据覆盖规则

为确保推荐结果覆盖全国各地知名大学，数据库需满足以下覆盖标准：

###### 各省大学数量要求

| 省份类型 | 最少数量 | 示例 |
|---------|---------|------|
| **北京** | 10所 | 清华、北大、北航、北理工、北邮、北交大等 |
| **上海** | 8所 | 复旦、上交、同济、华东师范、上财等 |
| **江苏** | 6所 | 南大、东南、南航、河海、南理工、苏州大学等 |
| **浙江** | 4所 | 浙大、浙江工业、杭电、宁波大学等 |
| **广东** | 4所 | 中大、华南理工、暨大、深大等 |
| **天津** | 4所 | 南开、天大、天医大、天师大等 |
| **湖北** | 4所 | 武大、华科、武汉理工、中南财经等 |
| **陕西** | 4所 | 西交大、西北工业、西电、陕西师范等 |
| **四川** | 3所 | 川大、电子科大、西南交通等 |
| **其他省份** | 3所 | 各省最好的985/211大学 |
| **总计** | **≥100所** | 覆盖全国31个省市自治区 |

###### 专业覆盖要求

每所大学需标注其王牌专业（3-5个），覆盖以下专业门类：

| 专业门类 | 核心专业示例 |
|---------|-------------|
| 工学 | 计算机科学与技术、人工智能、电子信息工程、机械工程 |
| 理学 | 数学、物理学、化学、统计学 |
| 医学 | 临床医学、口腔医学、药学 |
| 法学 | 法学、知识产权、社会学 |
| 经济学 | 金融学、经济学、会计学、工商管理 |
| 文学 | 汉语言文学、英语、新闻学 |
| 教育学 | 教育学、学前教育、体育教育 |

###### 数据更新规则

| 数据类型 | 更新频率 | 说明 |
|---------|---------|------|
| 大学基本信息 | 每年 | 985/211/双一流名单变化 |
| 录取分数线 | 每年 | 阳光高考最新数据 |
| 就业率 | 每年 | 麦可思就业质量报告 |
| 专业排名 | 每4年 | 教育部学科评估结果 |

###### 兜底规则

当所有场景都无法匹配到足够大学时，返回就业率最高的5所大学作为兜底。

##### 录取分数显示规则

在大学卡片中，需要清晰展示该大学的录取分数信息，包括最近一年和历史分数：

| 显示项目 | 格式 | 示例 |
|---------|------|------|
| **最近一年录取分数** | `{年份}年: {最低分}分` | 2024年: 680分 |
| **历史录取分数** | `{年份1}分 → {年份2}分` | 2023年659分 → 2022年652分 |
| **完整显示** | `{最近一年} ({历史})` | 2024年: 680分 (2023年659分 → 2022年652分) |

##### 录取分数数据来源

| 数据类型 | 来源 | 说明 |
|---------|------|------|
| 最近一年录取分 | 阳光高考 | 2024年各省录取分数线 |
| 历史录取分 | 阳光高考 | 2023年、2022年录取分数线 |

##### 录取分数显示UI规范

```
┌─────────────────────────────────────────────────────────────┐
│  清华大学           [985] [211]                    680分     │
│  北京 北京  •  就业率 99.2%                               │
│  本省江苏高校，人工智能专业实力较强                         │
│  📈 历史: 2023年659分 → 2022年652分                      │
└─────────────────────────────────────────────────────────────┘
```

**显示规则：**
1. **优先显示用户目标省份的录取分数**（如果该大学在该省有招生）
2. **如果目标省份没有招生**，显示该大学任意省份的最新录取分数
3. **历史分数**最多显示2年（过往2-3年）
4. **历史分数格式**：`年份1分 → 年份2分`
5. **鼠标悬停**可查看完整的历史分数信息

#### API接口设计

```yaml
# 获取专业详情（含推荐大学）
GET /api/v1/major/{major_id}/detail
Query Parameters:
  - province: string (可选，目标省份)
  - score: number (可选，预估分数)

Response:
  id: number
  name: string
  category: string
  heatIndex: number
  employmentRate: number
  avgSalary: string
  courses: string[]
  description: string
  careerProspects: string
  
  universities:
    - type: "score"
      name: "分数匹配大学"
      list:
        - name: string
          level: string
          admissionScore: string
          employmentRate: string
          location: string
          matchReason: string
    
    - type: "province"
      name: "同省优质大学"
      list:
        - ...
    
    - type: "national"
      name: "全国推荐大学"
      list:
        - ...

# 设置用户目标（存储到localStorage或账户）
POST /api/v1/user/target
Request:
  province: string
  score: number

Response:
  status: "success"
  message: "目标已设置"
```

#### 数据来源

| 数据类型 | 来源 | 说明 |
|---------|------|------|
| 大学基本信息 | 教育部公开数据 | 985/211/双一流名单 |
| 录取分数线 | 阳光高考 | 各高校本省录取分数线 |
| 就业率 | 麦可思报告 | 各专业毕业生就业质量报告 |
| 专业排名 | 教育部学科评估 | 第四次学科评估结果 |

#### 前端组件设计

```typescript
// 专业详情弹窗组件
const MajorDetailModal: React.FC<{
  major: Major;
  visible: boolean;
  onClose: () => void;
  userTarget: UserTarget | null;
  onUpdateTarget: (target: UserTarget) => void;
}> = ({ major, visible, onClose, userTarget, onUpdateTarget }) => {
  return (
    <Modal visible={visible} onClose={onClose}>
      {/* 标题区 */}
      <MajorHeader major={major} />
      
      {/* 数据概览 */}
      <DataOverview stats={major.stats} />
      
      {/* 用户目标设置 */}
      <TargetSelector 
        target={userTarget} 
        onUpdate={onUpdateTarget} 
      />
      
      {/* 个性化推荐大学 */}
      {major.universities?.map(group => (
        <UniversityGroup 
          key={group.type}
          title={group.name}
          universities={group.list}
        />
      ))}
      
      {/* 核心课程 */}
      <CourseTags courses={major.courses} />
      
      {/* 专业介绍 */}
      <Section title="💡 专业介绍" content={major.description} />
      
      {/* 就业前景 */}
      <Section title="🎯 就业前景" content={major.careerProspects} />
    </Modal>
  );
};
```

#### 实现优先级

| 优先级 | 功能 | 说明 |
|--------|------|------|
| P0 | 专业详情展示 | 基础信息展示 |
| P0 | 全国推荐大学 | 不依赖用户信息 |
| P0 | 核心课程/介绍/前景 | 基础内容展示 |
| P1 | 用户目标设置 | 省份+分数输入 |
| P1 | 分数匹配大学 | 根据分数推荐 |
| P1 | 同省优质大学 | 根据省份推荐 |
| P2 | 数据持久化 | localStorage存储 |
| P2 | 真实大学数据 | 接入阳光高考API |

#### 后续扩展方向

1. **智能志愿填报**：根据分数和省份，智能推荐"冲-稳-保"梯度院校
2. **历年分数线对比**：展示目标大学近5年录取分数线趋势
3. **专业对比**：支持多专业横向对比
4. **收藏功能**：收藏感兴趣的专业和大学
5. **高考后更新**：出分后更新推荐策略

### 数据可视化功能

#### 功能描述

将专业趋势和就业行情数据以直观的图表形式展示，帮助学生了解不同专业的发展前景。

#### 图表类型

| 图表类型 | 数据内容 | 交互功能 |
|---------|---------|---------|
| 折线图 | 专业分数线变化趋势 | 时间范围选择 |
| 柱状图 | 就业率对比 | 维度切换 |
| 饼图 | 学科门类分布 | 悬停详情 |
| 热力图 | 专业热度分布 | 区域筛选 |
| 雷达图 | 专业综合评估 | 动态更新 |

### 视频生成功能

#### 功能描述

基于专业数据和分析报告，自动生成专业讲解视频，帮助学生更直观地了解专业情况。

#### 技术方案

| 功能 | 技术选型 | 说明 |
|-----|---------|------|
| 脚本生成 | GPT-4 | 专业解读文稿 |
| 配音合成 | ElevenLabs | 语音生成 |
| 画面生成 | DALL-E 3 / Stable Diffusion | 配图生成 |
| 视频编排 | FFmpeg | 视频合成 |
| 字幕生成 | Whisper | 字幕同步 |

### 邮件服务功能

#### 功能描述

将学生感兴趣的专业信息、分析报告等发送到学生邮箱，方便学生后续查阅和参考。

#### 邮件类型

| 邮件类型 | 触发条件 | 内容 |
|---------|---------|------|
| 专业推荐 | 学生请求 | 专业列表、推荐理由 |
| 专业报告 | 定期生成 | 专业趋势、数据分析 |
| 重要提醒 | 关键节点 | 高考时间、志愿填报 |
| 系统通知 | 重要事件 | 账户变更、功能更新 |

---

## 用户体验设计

### 对话设计原则

1. **亲和力优先**：对话风格友好温暖，避免机械化的表达方式
2. **主动倾听**：在给出建议前，先确认理解学生的真实需求
3. **适度引导**：不直接告诉答案，而是通过提问引导学生思考
4. **情感共鸣**：理解学生的情绪状态，给予适当的支持和鼓励
5. **案例启发**：适时分享成功案例或故事，提供启发和参考

### 对话示例

```
学生：我不知道该选什么专业，感觉都很迷茫...

助手：我理解你的感受，面对人生重要选择感到迷茫是很正常的。
      （共情表达）
      
      其实，每个人都有自己的兴趣和优势。
      能告诉我，你在高中期间有没有哪门学科是你学起来特别开心，
      或者哪件事情是你做起来特别有成就感的吗？
      （开放性提问，引导学生思考）
```

### 无障碍设计

- 支持语音输入和语音输出
- 字体大小可调节
- 高对比度模式支持
- 语速可调节（TTS）

---

## 安全设计

### 认证授权

| 层级 | 技术 | 说明 |
|-----|------|------|
| 认证 | JWT + Refresh Token | 无状态认证 |
| 授权 | RBAC | 角色权限控制 |
| 传输 | HTTPS + TLS 1.3 | 加密传输 |
| 存储 | AES-256 | 敏感数据加密 |

### 安全措施

- SQL注入防护
- XSS攻击防护
- CSRFToken验证
- 请求频率限制
- API密钥轮换
- 敏感操作审计日志

---

## 性能要求

### 性能指标

| 指标 | 要求 | 说明 |
|-----|------|------|
| API响应时间 | < 500ms | P99 < 1s |
| 语音识别延迟 | < 2s | 端到端 |
| 语音合成延迟 | < 1s | 首字节时间 |
| 页面加载时间 | < 3s | 首屏渲染 |
| 并发用户数 | > 1000 | 同时在线 |
| 可用性 | > 99.9% | 年度可用性 |

### 优化策略

- 多级缓存（CDN → Nginx → Redis → 本地）
- 数据库读写分离
- 消息队列削峰
- 服务横向扩展
- 静态资源CDN加速

### 缓存策略

#### 学科介绍内容缓存

为了平衡内容时效性和系统性能，对专业介绍内容采用基于学科热度的差异化缓存策略：

| 学科类型 | 缓存时间 | 示例专业 |
|---------|---------|---------|
| **热门学科** | 24小时 | 人工智能、计算机科学与技术、金融学、临床医学、法学等 |
| **普通学科** | 72小时 | 大多数传统专业 |

**热门学科判定标准：**
- AI相关：人工智能、机器学习、深度学习、自然语言处理、计算机视觉
- 新兴技术：数据科学与大数据技术、云计算、物联网、区块链、网络安全
- 热门工程：计算机科学与技术、软件工程、电子信息工程、自动化
- 热门经济管理：金融学、经济学、工商管理
- 热门医学：临床医学、口腔医学
- 热门法学：法学

**缓存实现细节：**
- 首次请求时动态生成内容并写入缓存
- 后续请求检查缓存过期时间，未过期直接返回缓存
- 缓存过期后自动重新生成新内容
- 支持手动清除缓存（用于紧急更新）

**API响应字段：**
```json
{
  "cached": true/false,          // 是否命中缓存
  "is_hot_major": true/false,    // 是否为热门学科
  "cache_ttl_hours": 24/72,      // 缓存有效期（小时）
  "cache_hit": true/false        // 是否从缓存获取
}
```

**缓存存储位置：**
- 本地文件系统：`/tmp/doc_processor_cache/major_intro_cache.json`
- Redis（可选扩展）：用于多实例共享缓存

---

## 监控告警

### 监控指标

| 维度 | 指标 | 告警阈值 |
|-----|------|---------|
| 系统 | CPU > 80% | 警告 |
| 系统 | 内存 > 85% | 警告 |
| 系统 | 磁盘 > 90% | 严重 |
| 应用 | 错误率 > 1% | 警告 |
| 应用 | 响应时间 > 2s | 警告 |
| 业务 | 语音识别失败率 > 5% | 警告 |
| 业务 | 对话成功率 < 95% | 警告 |

### 监控工具

- **Prometheus**：指标采集和存储
- **Grafana**：可视化面板
- **ELK Stack**：日志收集和分析
- **Alertmanager**：告警管理

---

## 项目文档结构

```
major-guidance-app/
├── .opencode/
│   ├── opencode.json           # 项目配置
│   ├── agent/                   # Agent定义
│   │   ├── coordinator.md
│   │   ├── frontend.md
│   │   ├── backend.md
│   │   ├── ai-ml.md
│   │   ├── data.md
│   │   └── devops.md
│   └── skill/                   # Skill定义
│       ├── coordinator/
│       ├── frontend/
│       ├── backend/
│       ├── ai-ml/
│       ├── data/
│       ├── devops/
│       ├── document-processor/
│       └── video-processor/
├── frontend/                    # 前端项目
│   ├── web/                     # Web端
│   │   ├── src/
│   │   ├── public/
│   │   ├── Dockerfile
│   │   └── README.md
│   ├── mobile/                  # 移动端
│   │   ├── src/
│   │   ├── ios/
│   │   ├── android/
│   │   ├── Dockerfile
│   │   └── README.md
│   └── miniprogram/             # 小程序
│       ├── src/
│       └── README.md
├── backend/                     # 后端项目
│   ├── api-gateway/             # API网关
│   ├── user-service/            # 用户服务
│   ├── voice-service/           # 语音服务
│   ├── chat-service/            # 对话服务
│   ├── crawler-service/         # 爬虫服务
│   ├── major-service/           # 专业服务
│   ├── recommendation-service/  # 推荐服务
│   ├── analytics-service/       # 分析服务
│   ├── video-service/           # 视频服务
│   ├── email-service/           # 邮件服务
│   ├── knowledge-base/          # 知识库
│   └── notification-service/    # 通知服务
├── database/
│   ├── migrations/
│   ├── seeds/
│   └── README.md
├── docker/
│   ├── docker-compose.yml
│   ├── docker-compose.dev.yml
│   ├── docker-compose.prod.yml
│   ├── docker-compose.infra.yml
│   ├── .env.example
│   └── nginx/
│       ├── nginx.conf
│       └── README.md
├── docs/
│   ├── ARCHITECTURE.md          # 架构文档
│   ├── API.md                   # API文档
│   ├── DEPLOY.md                # 部署文档
│   └── DEVELOPMENT.md           # 开发文档
├── scripts/
│   ├── setup.sh
│   ├── deploy.sh
│   ├── backup.sh
│   └── manage.sh
├── .gitignore
├── .editorconfig
├── .eslintrc
├── .prettierrc
├── Dockerfile
├── docker-compose.yml
├── Makefile
├── README.md
└── PROJECT_PLAN.md              # 项目规划（本文档）
```

---

## 附录

### 参考资料

- [OpenAI API文档](https://platform.openai.com/docs)
- [Whisper文档](https://github.com/openai/whisper)
- [ElevenLabs API](https://elevenlabs.io/docs/api-docs)
- [FastAPI文档](https://fastapi.tiangolo.com)
- [LangChain文档](https://python.langchain.com)
- [Scrapy文档](https://docs.scrapy.org)
- [Docker官方文档](https://docs.docker.com)
- [Kafka文档](https://kafka.apache.org/documentation)

### 术语表

| 术语 | 定义 |
|-----|------|
| ASR | Automatic Speech Recognition，语音识别 |
| TTS | Text-to-Speech，语音合成 |
| LLM | Large Language Model，大语言模型 |
| RBAC | Role-Based Access Control，基于角色的访问控制 |
| JWT | JSON Web Token，JSON Web令牌 |
| CDN | Content Delivery Network，内容分发网络 |

---

**文档版本**：1.0  
**创建日期**：2026-01-22  
**最后更新**：2026-01-22
