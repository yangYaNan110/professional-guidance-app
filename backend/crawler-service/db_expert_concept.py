#!/usr/bin/env python3
"""
DB-Expertï¼šæ›´æ–°æ•°æ®åº“ç»“æ„æ”¯æŒä¸“ä¸šæ¦‚å¿µæ•°æ®
å°†çˆ¬å–çš„ä¸“ä¸šæ¦‚å¿µæ•°æ®å†™å…¥æ•°æ®åº“å¹¶ä¼˜åŒ–å­˜å‚¨ç»“æ„
"""

import psycopg2
import json
import os
from datetime import datetime
from typing import Dict, Any, List

class DBExpert:
    def __init__(self):
        # ä½¿ç”¨æ•°æ®åº“è¿æ¥å‚æ•°
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': os.getenv('DB_NAME', 'employment'),
            'user': 'postgres',
            'password': os.getenv('POSTGRES_PASSWORD', 'your_password'),
            'host': os.getenv('DB_HOST', 'localhost'),  # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': os.getenv('DB_NAME', 'employment'),
            'user': 'postgres',
            'password': os.getenv('POSTGRES_PASSWORD', 'your_password')
        }
        self.connection = None
        print("ğŸ”— æ•°æ®åº“é…ç½®åˆå§‹åŒ–å®Œæˆ")
    
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.connection = psycopg2.connect(**self.db_config)
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            print("ğŸ”Œ æ•°æ®åº“è¿æ¥å·²æ–­å¼€")
    
    def create_major_concept_table(self):
        """åˆ›å»ºä¸“ä¸šæ¦‚å¿µè¡¨"""
        create_table_sql = """
        -- åˆ›å»ºä¸“ä¸šæ¦‚å¿µè¡¨
        CREATE TABLE IF NOT EXISTS major_concepts (
            id SERIAL PRIMARY KEY,
            major_name VARCHAR(200) NOT NULL,
            concept_type VARCHAR(50) NOT NULL, -- 'origin', 'development_history', 'major_events', 'current_status', 'future_prospects'
            title VARCHAR(500),  -- æ—¶é—´èŠ‚ç‚¹æˆ–äº‹ä»¶æ ‡é¢˜
            content TEXT NOT NULL,  -- è¯¦ç»†å†…å®¹
            year INTEGER,  -- ç›¸å…³å¹´ä»½ï¼ˆå¯é€‰ï¼‰
            sort_order INTEGER DEFAULT 0,
            source_url VARCHAR(500),
            data_quality VARCHAR(20) DEFAULT 'high',
            crawled_at TIMESTAMP DEFAULT NOW(),
            updated_at TIMESTAMP DEFAULT NOW()
        );
        
        -- åˆ›å»ºç´¢å¼•
        CREATE INDEX IF NOT EXISTS idx_major_concepts_major_name ON major_concepts(major_name);
        CREATE INDEX IF NOT EXISTS idx_major_concepts_type ON major_concepts(concept_type);
        CREATE INDEX IF NOT EXISTS idx_major_concepts_year ON major_concepts(year);
        CREATE INDEX IF NOT EXISTS idx_major_concepts_crawled_at ON major_concepts(crawled_at DESC);
        
        -- åˆ›å»ºè§¦å‘å™¨è‡ªåŠ¨æ›´æ–°updated_at
        CREATE OR REPLACE FUNCTION update_major_concepts_updated_at()
        RETURNS TRIGGER AS $$
        BEGIN
            NEW.updated_at = NOW();
            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
        
        CREATE TRIGGER update_major_concepts_updated_at_trigger
            BEFORE UPDATE ON major_concepts
            FOR EACH ROW
            EXECUTE FUNCTION update_major_concepts_updated_at();
        """
        
        cursor = self.connection.cursor()
        try:
            cursor.execute(create_table_sql)
            self.connection.commit()
            print("âœ… ä¸“ä¸šæ¦‚å¿µè¡¨åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            self.connection.rollback()
            print(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            raise
        finally:
            cursor.close()
    
    def insert_concept_data(self, concept_data: Dict[str, Any]):
        """æ’å…¥æ¦‚å¿µæ•°æ®"""
        insert_sql = """
        INSERT INTO major_concepts (
            major_name, concept_type, title, content, year, sort_order, source_url, data_quality, crawled_at
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        cursor = self.connection.cursor()
        try:
            major_name = concept_data["major_name"]
            
            # æ’å…¥ä¸“ä¸šèµ·æº
            if "origin" in concept_data:
                cursor.execute(insert_sql, (
                    major_name, "origin", "ä¸“ä¸šèµ·æº", concept_data["origin"], 
                    self._extract_year_from_text(concept_data["origin"]), 1, 
                    "https://gaokao.chsi.com.cn", "high", datetime.now()
                ))
            
            # æ’å…¥å‘å±•å†å²
            if "development_history" in concept_data:
                for i, event in enumerate(concept_data["development_history"]):
                    cursor.execute(insert_sql, (
                        major_name, "development_history", event, event, 
                        self._extract_year_from_text(event), i + 1, 
                        "https://gaokao.chsi.com.cn", "high", datetime.now()
                    ))
            
            # æ’å…¥é‡å¤§äº‹ä»¶
            if "major_events" in concept_data:
                for i, event in enumerate(concept_data["major_events"]):
                    cursor.execute(insert_sql, (
                        major_name, "major_events", event, event, 
                        self._extract_year_from_text(event), i + 1, 
                        "https://gaokao.chsi.com.cn", "high", datetime.now()
                    ))
            
            # æ’å…¥ç°çŠ¶
            if "current_status" in concept_data:
                cursor.execute(insert_sql, (
                    major_name, "current_status", "å‘å±•ç°çŠ¶", concept_data["current_status"], 
                    2024, 1, 
                    "https://www.eol.cn", "high", datetime.now()
                ))
            
            # æ’å…¥æœªæ¥å‰æ™¯
            if "future_prospects" in concept_data:
                cursor.execute(insert_sql, (
                    major_name, "future_prospects", "æœªæ¥å‰æ™¯", concept_data["future_prospects"], 
                    2025, 1, 
                    "https://gaokao.chsi.com.cn", "high", datetime.now()
                ))
            
            self.connection.commit()
            print(f"âœ… æ’å…¥ {major_name} çš„æ¦‚å¿µæ•°æ®æˆåŠŸ")
            
        except Exception as e:
            self.connection.rollback()
            print(f"âŒ æ’å…¥æ•°æ®å¤±è´¥: {e}")
            raise
        finally:
            cursor.close()
    
    def _extract_year_from_text(self, text: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–å¹´ä»½"""
        import re
        # åŒ¹é…4ä½æ•°å­—å¹´ä»½
        year_match = re.search(r'(\d{4})', text)
        return int(year_match.group(1)) if year_match else None
    
    def create_indexes(self):
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•"""
        index_sql = """
        -- åˆ›å»ºå¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
        CREATE INDEX IF NOT EXISTS idx_major_concepts_composite 
        ON major_concepts(major_name, concept_type, year);
        
        -- åˆ†ææŸ¥è¯¢çš„ä¼˜åŒ–ç´¢å¼•
        CREATE INDEX IF NOT EXISTS idx_major_concepts_analysis 
        ON major_concepts(concept_type, crawled_at DESC);
        """
        
        cursor = self.connection.cursor()
        try:
            cursor.execute(index_sql)
            self.connection.commit()
            print("âœ… æ€§èƒ½ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        finally:
            cursor.close()
    
    def validate_data_quality(self) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è´¨é‡"""
        validation_sql = """
        SELECT 
            COUNT(*) as total_concepts,
            COUNT(CASE WHEN concept_type = 'origin' THEN 1 END) as origin_count,
            COUNT(CASE WHEN concept_type = 'development_history' THEN 1 END) as history_count,
            COUNT(CASE WHEN concept_type = 'major_events' THEN 1 END) as events_count,
            COUNT(CASE WHEN concept_type = 'current_status' THEN 1 END) as status_count,
            COUNT(CASE WHEN concept_type = 'future_prospects' THEN 1 END) as prospects_count,
            COUNT(CASE WHEN year IS NOT NULL THEN 1 END) as with_year_count
        FROM major_concepts;
        """
        
        cursor = self.connection.cursor()
        try:
            cursor.execute(validation_sql)
            result = cursor.fetchone()
            validation_report = {
                "total_concepts": result[0],
                "data_types": {
                    "origin": result[1],
                    "development_history": result[2],
                    "major_events": result[3],
                    "current_status": result[4],
                    "future_prospects": result[5]
                },
                "quality_score": min(100, result[0] // 5 * 10)  if result[0] > 0 else 0,
                "with_year_data": result[6],
                "validation_time": datetime.now().isoformat()
            }
            print(f"âœ… æ•°æ®éªŒè¯å®Œæˆ: {validation_report}")
            return validation_report
        except Exception as e:
            print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return {"error": str(e)}
        finally:
            cursor.close()
    
    def process_all_data(self, data_file: str = "major_concept_data.json"):
        """å¤„ç†æ‰€æœ‰çˆ¬å–çš„æ•°æ®"""
        # è¯»å–çˆ¬å–çš„æ•°æ®
        data_file_path = os.path.join(os.path.dirname(__file__), data_file)
        
        if not os.path.exists(data_file_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file_path}")
            return None
        
        with open(data_file_path, 'r', encoding='utf-8') as f:
            crawl_data = json.load(f)
        
        if not crawl_data.get("success") or not crawl_data.get("data"):
            print("âŒ çˆ¬å–æ•°æ®æ ¼å¼é”™è¯¯")
            return None
        
        print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(crawl_data['data'])} ä¸ªä¸“ä¸šçš„æ¦‚å¿µæ•°æ®...")
        
        # åˆ›å»ºè¡¨ç»“æ„
        self.create_major_concept_table()
        
        # æ’å…¥æ•°æ®
        for concept_data in crawl_data["data"]:
            try:
                self.insert_concept_data(concept_data)
            except Exception as e:
                print(f"âš ï¸  æ’å…¥æ•°æ®å¤±è´¥: {concept_data.get('major_name', 'Unknown')}: {e}")
                continue
        
        # åˆ›å»ºæ€§èƒ½ç´¢å¼•
        self.create_indexes()
        
        # éªŒè¯æ•°æ®è´¨é‡
        validation_report = self.validate_data_quality()
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        process_report = {
            "success": True,
            "data_file": data_file_path,
            "processed_majors": len(crawl_data["data"]),
            "total_concepts": validation_report.get("total_concepts", 0),
            "quality_score": validation_report.get("quality_score", 0),
            "data_types": validation_report.get("data_types", {}),
            "process_time": datetime.now().isoformat(),
            "message": f"æˆåŠŸå¤„ç† {len(crawl_data['data'])} ä¸ªä¸“ä¸šçš„æ¦‚å¿µæ•°æ®"
        }
        
        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_file = data_file_path.replace('.json', '_process_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(process_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ å¤„ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return process_report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ DB-Expert å¼€å§‹å¤„ç†ä¸“ä¸šæ¦‚å¿µæ•°æ®...")
    
    expert = DBExpert()
    
    try:
        # è¿æ¥æ•°æ®åº“
        expert.connect()
        
        # å¤„ç†æ•°æ®
        report = expert.process_all_data()
        
        if report:
            print("âœ… æ•°æ®åº“æ›´æ–°å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   - å¤„ç†ä¸“ä¸šæ•°: {report.get('processed_majors', 0)}")
            print(f"   - æ€»æ¦‚å¿µæ¡ç›®: {report.get('total_concepts', 0)}")
            print(f"   - æ•°æ®è´¨é‡è¯„åˆ†: {report.get('quality_score', 0)}/100")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    finally:
        expert.disconnect()

if __name__ == "__main__":
    main()